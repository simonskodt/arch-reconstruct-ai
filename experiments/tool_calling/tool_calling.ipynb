{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5d937",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0477cd",
   "metadata": {},
   "source": [
    "## Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b353a",
   "metadata": {},
   "source": [
    "#### Clone a repository based on url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1385bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from git import Repo, GitCommandError\n",
    "from langchain.tools import tool\n",
    "import os, shutil\n",
    "\n",
    "\n",
    "@tool(\"git_clone\")\n",
    "def git_clone_tool(\n",
    "    repo_url: str,\n",
    "    dest: str,\n",
    "    branch: Optional[str] = None,\n",
    "    overwrite: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Clone a Git repository into ./repositories/{dest} using GitPython.\n",
    "\n",
    "    Args:\n",
    "        repo_url: HTTPS or SSH URL of the repository.\n",
    "        dest: Name of the destination folder for the clone inside ./repositories/.\n",
    "        branch: Optional branch to check out.\n",
    "        overwrite: If True, overwrite existing destination folder.\n",
    "    Returns:\n",
    "        A dict with success (bool), dest (str), and error/stdout messages.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Ensure repositories/ root exists\n",
    "        root_dir = os.path.join(os.getcwd(), \"repositories\")\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "        # Full destination path inside repositories/\n",
    "        full_dest = os.path.join(root_dir, dest)\n",
    "\n",
    "        # Handle overwrite\n",
    "        if os.path.exists(full_dest):\n",
    "            if overwrite:\n",
    "                shutil.rmtree(full_dest)\n",
    "            else:\n",
    "                return {\"success\": False, \"error\": f\"Destination {full_dest} already exists.\"}\n",
    "\n",
    "\n",
    "        # Clone options\n",
    "        kwargs = {}\n",
    "        if branch:\n",
    "            kwargs[\"branch\"] = branch\n",
    "            full_dest = f\"{full_dest}/{branch}\"\n",
    "\n",
    "        os.makedirs(full_dest, exist_ok=True)\n",
    "        repo = Repo.clone_from(repo_url, full_dest, **kwargs)\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"dest\": full_dest,\n",
    "            \"branch\": repo.active_branch.name if not repo.head.is_detached else \"detached\",\n",
    "            \"error\": None,\n",
    "        }\n",
    "    except GitCommandError as e:\n",
    "        return {\"success\": False, \"dest\": dest, \"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"dest\": dest, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4be9f",
   "metadata": {},
   "source": [
    "#### Add Github MCP server based on url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "059673ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from langchain.tools import tool\n",
    "from experiments.utils.mcp_client_factory import load_mcp_config, save_mcp_config, create_mcp_client_from_config\n",
    "\n",
    "\n",
    "@tool(\"add_mcp_server\")\n",
    "def add_mcp_server_tool(\n",
    "    name: str,\n",
    "    url: str,\n",
    "    transport: str = \"streamable_http\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Add a new MCP server to the configuration.\n",
    "    \n",
    "    Args:\n",
    "        name: Name identifier for the MCP server\n",
    "        url: URL of the MCP server\n",
    "        transport: Transport type (default: \"streamable_http\")\n",
    "    Returns:\n",
    "        Dict with success status and current config\n",
    "\n",
    "    Note: \n",
    "        MCP tools accessed via clients are not hot reloaded or dynamically  updated,\n",
    "        a new agent or tool instance has to be  \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load existing config\n",
    "        config = load_mcp_config()\n",
    "        \n",
    "        # Add new server\n",
    "        config[name] = {\n",
    "            \"url\": url,\n",
    "            \"transport\": transport\n",
    "        }\n",
    "    \n",
    "        save_mcp_config(config)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Added MCP server '{name}'\",\n",
    "            \"config\": config\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"config\": {}\n",
    "        }\n",
    "    \n",
    "@tool(\"add_github_repository_as_mcp_server\")\n",
    "def add_github_repository_as_mcp_tool(repo_url: str, server_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Add a GitHub repository as an MCP server using gitmcp.io.\n",
    "    \n",
    "    Args:\n",
    "        repo_url: GitHub repository URL (e.g., https://github.com/owner/repo)\n",
    "        server_name: Name identifier for the MCP server\n",
    "    Returns:\n",
    "        Dict with success status and current config\n",
    "    \"\"\"\n",
    "    # Extract the repository path from the GitHub URL\n",
    "    if \"github.com/\" in repo_url:\n",
    "        # Extract everything after github.com/\n",
    "        repo_path = repo_url.split(\"github.com/\", 1)[1]\n",
    "        # Remove .git suffix if present\n",
    "        if repo_path.endswith(\".git\"):\n",
    "            repo_path = repo_path[:-4]\n",
    "        gitmcp_url = f\"https://gitmcp.io/{repo_path}\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid GitHub repository URL\")\n",
    "    \n",
    "    tool_input = {\n",
    "        \"name\": server_name,\n",
    "        \"url\": gitmcp_url,\n",
    "    }\n",
    "\n",
    "    return add_mcp_server_tool.invoke(tool_input)\n",
    "\n",
    "    \n",
    "\n",
    "@tool(\"remove_mcp_server\")\n",
    "def remove_mcp_server_tool(\n",
    "    name: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove an MCP server from the configuration.\n",
    "    \n",
    "    Args:\n",
    "        name: Name identifier of the MCP server to remove\n",
    "    Returns:\n",
    "        Dict with success status and current config\n",
    "\n",
    "    Note: \n",
    "        MCP tools accessed via clients are not hot reloaded or dynamically  updated,\n",
    "        a new agent or tool instance has to be \n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = load_mcp_config()\n",
    "        \n",
    "        if name not in config:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"MCP server '{name}' not found\",\n",
    "                \"config\": config\n",
    "            }\n",
    "        \n",
    "        del config[name]\n",
    "        save_mcp_config(config)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Removed MCP server '{name}'\",\n",
    "            \"config\": config\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"config\": {}\n",
    "        }\n",
    "\n",
    "@tool(\"list_mcp_servers\")\n",
    "def list_mcp_servers() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    List all configured MCP servers.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with success status and list of servers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = load_mcp_config()\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"servers\": list(config.keys()),\n",
    "            \"config\": config\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"config\": {}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iqNmvENZzhZ7Wh3kntQPXqkJ', 'function': {'arguments': '{\"repo_url\":\"https://github.com/simonskodt/arch-reconstruct-ai\",\"dest\":\"arch-reconstruct-ai\",\"branch\":null,\"overwrite\":false}', 'name': 'git_clone'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 308, 'prompt_tokens': 511, 'total_tokens': 819, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo4ayGmdBLSAkgAUugCxyrMtnlJ7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--277e2183-d669-49fd-844c-d2a1f3f364b6-0', tool_calls=[{'name': 'git_clone', 'args': {'repo_url': 'https://github.com/simonskodt/arch-reconstruct-ai', 'dest': 'arch-reconstruct-ai', 'branch': None, 'overwrite': False}, 'id': 'call_iqNmvENZzhZ7Wh3kntQPXqkJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 511, 'output_tokens': 308, 'total_tokens': 819, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='{\"success\": true, \"dest\": \"/Users/thomas/Desktop/arch-reconstruct-ai/experiments/tool-calling/repositories/arch-reconstruct-ai\", \"branch\": \"main\", \"error\": null}', name='git_clone', id='850b5a5c-b43c-4d8d-afa8-0fa1209ba2ec', tool_call_id='call_iqNmvENZzhZ7Wh3kntQPXqkJ')]}}\n",
      "{'agent': {'messages': [AIMessage(content='Cloning complete.\\n\\n- Destination: /Users/thomas/Desktop/arch-reconstruct-ai/experiments/tool-calling/repositories/arch-reconstruct-ai\\n- Branch checked out: main\\n- Status: success (no errors)\\n\\nWould you like me to do any of the following?\\n- List the top-level files and folders to get an overview\\n- Open and summarize the README and/or important docs\\n- Set up a virtual environment and install dependencies\\n- Run a quick check or a sample script from the repo\\n- Inspect specific parts (e.g., notebooks, experiments, or modules)\\n\\nTell me what you want to explore next.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 905, 'prompt_tokens': 611, 'total_tokens': 1516, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 768, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo4erMPAnvcfbisGZ7Ny38HAinfV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e90b2989-8340-410d-b832-9602c985684f-0', usage_metadata={'input_tokens': 611, 'output_tokens': 905, 'total_tokens': 1516, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 768}})]}}\n",
      "{'messages': [HumanMessage(content='Can you list the MCP servers available', additional_kwargs={}, response_metadata={}, id='69a97777-868b-45c7-adc4-1c65e5b0d72a'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lckmEivf6jSR4N2V2m5J54ez', 'function': {'arguments': '{}', 'name': 'list_mcp_servers'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 497, 'total_tokens': 582, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo4o0yiUHThct7kO0xUPvkTO4gml', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b4046b50-80f7-491c-9a3f-2b4b8edc2b81-0', tool_calls=[{'name': 'list_mcp_servers', 'args': {}, 'id': 'call_lckmEivf6jSR4N2V2m5J54ez', 'type': 'tool_call'}], usage_metadata={'input_tokens': 497, 'output_tokens': 85, 'total_tokens': 582, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}), ToolMessage(content='{\"success\": true, \"servers\": [], \"config\": {}}', name='list_mcp_servers', id='87874eba-0846-46ba-b618-8abf1e1cb360', tool_call_id='call_lckmEivf6jSR4N2V2m5J54ez'), AIMessage(content='Currently there are no MCP servers configured.\\n\\nWhat would you like to do next? Options:\\n- Add a new MCP server: provide name and URL (and optional transport; default is streamable_http)\\n  - Example: add_mcp_server with { name: \"my-server\", url: \"https://example.com/mcp\" }\\n- Link a GitHub repository as an MCP server: provide repo_url and server_name\\n  - Example: add_github_repository_as_mcp_server with { repo_url: \"https://github.com/owner/repo\", server_name: \"repo-mcp\" }\\n- After adding, I can list again to confirm.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 461, 'prompt_tokens': 539, 'total_tokens': 1000, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo4uGahDChxbonNdp71CKUhZQQiI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e31ef77c-796b-41cb-b5f3-0c7330ce52cb-0', usage_metadata={'input_tokens': 539, 'output_tokens': 461, 'total_tokens': 1000, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}})]}\n",
      "{'messages': [HumanMessage(content='Can you take the following github repository: https://github.com/simonskodt/arch-reconstruct-ai, and make it into a MCP server', additional_kwargs={}, response_metadata={}, id='4fc9b395-0112-4f98-b252-fb5ea7bcf6bd'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_agCCfhyUkMoE5nYNloudJWdR', 'function': {'arguments': '{\"repo_url\":\"https://github.com/simonskodt/arch-reconstruct-ai\",\"server_name\":\"arch-reconstruct-ai-mcp\"}', 'name': 'add_github_repository_as_mcp_server'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 949, 'prompt_tokens': 520, 'total_tokens': 1469, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 896, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo4yYhnw5r3O7R8yblHiHJBdBM0M', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d2179dfb-2a9e-4a76-955e-5ab9cc88cc13-0', tool_calls=[{'name': 'add_github_repository_as_mcp_server', 'args': {'repo_url': 'https://github.com/simonskodt/arch-reconstruct-ai', 'server_name': 'arch-reconstruct-ai-mcp'}, 'id': 'call_agCCfhyUkMoE5nYNloudJWdR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 520, 'output_tokens': 949, 'total_tokens': 1469, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 896}}), ToolMessage(content='{\"success\": true, \"message\": \"Added MCP server \\'arch-reconstruct-ai-mcp\\'\", \"config\": {\"arch-reconstruct-ai-mcp\": {\"url\": \"https://gitmcp.io/simonskodt/arch-reconstruct-ai\", \"transport\": \"streamable_http\"}}}', name='add_github_repository_as_mcp_server', id='3a7289fd-15ec-461d-a350-2768781edb85', tool_call_id='call_agCCfhyUkMoE5nYNloudJWdR'), AIMessage(content='All set. I’ve turned the repository into an MCP server.\\n\\nWhat I did\\n- Added MCP server for the repo: https://github.com/simonskodt/arch-reconstruct-ai\\n- Server name: arch-reconstruct-ai-mcp\\n- MCP URL: https://gitmcp.io/simonskodt/arch-reconstruct-ai\\n- Transport: streamable_http\\n\\nCurrent config (high level)\\n- arch-reconstruct-ai-mcp\\n  - url: https://gitmcp.io/simonskodt/arch-reconstruct-ai\\n  - transport: streamable_http\\n\\nNext steps (optional)\\n- Verify connectivity: open https://gitmcp.io/simonskodt/arch-reconstruct-ai in a browser or use an MCP client to test.\\n- List all configured MCP servers if you want to see others: I can run a list for you.\\n- Clone the repository locally for testing in the MCP environment:\\n  - If you want, I can clone it into ./repositories/arch-reconstruct-ai using GitPython.\\n- If you’d like a different server name or a different transport, I can adjust and reconfigure.\\n\\nWould you like me to clone the repository into the environment for testing, or list existing MCP servers to confirm the setup?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1292, 'prompt_tokens': 645, 'total_tokens': 1937, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1024, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo55OsCNzltFyQF7DYQdiCV3QGD9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6cfc6363-8b43-4c9f-bafe-3e59e2c59ae3-0', usage_metadata={'input_tokens': 645, 'output_tokens': 1292, 'total_tokens': 1937, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1024}})]}\n",
      "add_github_repository_as_mcp_server\n",
      "add_mcp_server\n",
      "remove_mcp_server\n",
      "list_mcp_servers\n",
      "git_clone\n",
      "fetch_arch_reconstruct_docs\n",
      "search_arch_docs\n",
      "search_arch_code\n",
      "fetch_generic_url_content\n",
      "{'messages': [HumanMessage(content='Can you list the MCP servers available', additional_kwargs={}, response_metadata={}, id='b6f5c0db-4a60-4bf5-b240-870f234526c5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KpYqAsQjbp0NUCMInAWasUUF', 'function': {'arguments': '{}', 'name': 'list_mcp_servers'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 748, 'total_tokens': 897, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo5HZkZjjRBgAtCg9bxJZAGyKfei', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5874247b-332c-41d3-a0dc-8ea2dd6d68f8-0', tool_calls=[{'name': 'list_mcp_servers', 'args': {}, 'id': 'call_KpYqAsQjbp0NUCMInAWasUUF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 748, 'output_tokens': 149, 'total_tokens': 897, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}), ToolMessage(content='{\"success\": true, \"servers\": [\"arch-reconstruct-ai-mcp\"], \"config\": {\"arch-reconstruct-ai-mcp\": {\"url\": \"https://gitmcp.io/simonskodt/arch-reconstruct-ai\", \"transport\": \"streamable_http\"}}}', name='list_mcp_servers', id='2ff68457-f1b6-41fe-84c3-7132762b2921', tool_call_id='call_KpYqAsQjbp0NUCMInAWasUUF'), AIMessage(content='Here are the available MCP servers:\\n\\n- arch-reconstruct-ai-mcp\\n  - URL: https://gitmcp.io/simonskodt/arch-reconstruct-ai\\n  - Transport: streamable_http\\n\\nCurrent configuration: arch-reconstruct-ai-mcp is configured with the above URL and transport.\\n\\nWould you like to add more MCP servers or modify this configuration? I can help with adding a new server or linking a GitHub repository as an MCP server.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 487, 'prompt_tokens': 833, 'total_tokens': 1320, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CLo5dXaLpwcvhHX51AEei77YtwD2u', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3574429a-140b-4814-9421-8f36875880f4-0', usage_metadata={'input_tokens': 833, 'output_tokens': 487, 'total_tokens': 1320, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to sys.path so 'experiments' can be imported\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "from experiments.utils.agent_factory import create_agent_with_valid_tools\n",
    "\n",
    "tools = [add_github_repository_as_mcp_tool, add_mcp_server_tool, remove_mcp_server_tool, list_mcp_servers, git_clone_tool]\n",
    "\n",
    "agent = create_agent_with_valid_tools(\n",
    "    \"openai:gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    prompt=\"\"\"Act as an assistant.\n",
    "                When using tools:\n",
    "                - Use tools if relevant before answering.\n",
    "            \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "stream = agent.astream({\"messages\": [HumanMessage(\"Can you clone the github repository: https://github.com/simonskodt/arch-reconstruct-ai\")]})\n",
    "async for chunk in stream:\n",
    "    print(chunk)\n",
    "\n",
    "\n",
    "result = await agent.ainvoke({\"messages\": [HumanMessage(\"Can you list the MCP servers available\")]})\n",
    "print(result)\n",
    "\n",
    "result = await agent.ainvoke({\"messages\": [HumanMessage(\"Can you take the following github repository: https://github.com/simonskodt/arch-reconstruct-ai, and make it into a MCP server\")]})\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client = create_mcp_client_from_config()\n",
    "mcp_tools = await client.get_tools() \n",
    "tools += mcp_tools\n",
    "\n",
    "[print(tool.name) for tool in tools]\n",
    "\n",
    "agent = create_agent_with_valid_tools(\n",
    "    \"openai:gpt-5-nano\",\n",
    "    tools=tools, # Tools cannot be dynamically  or hot reloaded?, agent has to be recreated  \n",
    "    prompt=\"\"\"Act as an assistant.\n",
    "                When using tools:\n",
    "                - Use tools if relevant before answering.\n",
    "            \"\"\"\n",
    ")\n",
    "result = await agent.ainvoke({\"messages\": [HumanMessage(\"Can you list the MCP servers available\")]})\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f0239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-08 01:17:44.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1737\u001b[0m | Exception in execute request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agent = \u001b[43mcreate_agent\u001b[49m(\n",
      "\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mopenai:gpt-5-nano\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m      3\u001b[39m     tools=[list_current_directory, change_directory],\n",
      "\u001b[32m      4\u001b[39m     prompt=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAct as an assistant, that can navigate the file system using the tools provided.\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m      5\u001b[39m )\n",
      "\u001b[32m      7\u001b[39m result = agent.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(\u001b[33m\"\u001b[39m\u001b[33mCan you list the current directory?\u001b[39m\u001b[33m\"\u001b[39m)]})\n",
      "\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'create_agent' is not defined\n",
      "\u001b[32m2025-10-08 01:18:07.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1737\u001b[0m | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-10-08 01:18:10.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1737\u001b[0m | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-10-08 01:40:54.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m89\u001b[0m | Starting ingestion process | {\"source\":\"./repositories/zeeguu-api\"}\n",
      "\u001b[32m2025-10-08 01:40:54.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m105\u001b[0m | Processing local directory | {\"source\":\"./repositories/zeeguu-api\"}\n",
      "\u001b[32m2025-10-08 01:40:54.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m134\u001b[0m | Starting local directory processing\n",
      "\u001b[32m2025-10-08 01:40:54.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m140\u001b[0m | Processing files and generating output\n",
      "\u001b[32m2025-10-08 01:40:54.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m44\u001b[0m | Starting file ingestion | {\"slug\":\"./repositories/zeeguu-api\",\"subpath\":\"/\",\"local_path\":\"/Users/thomas/Desktop/arch-reconstruct-ai/experiments/tool_calling/repositories/zeeguu-api\",\"max_file_size\":10485760}\n",
      "\u001b[32m2025-10-08 01:40:54.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m96\u001b[0m | Processing directory | {\"directory_path\":\"/Users/thomas/Desktop/arch-reconstruct-ai/experiments/tool_calling/repositories/zeeguu-api\"}\n",
      "\u001b[32m2025-10-08 01:40:54.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m109\u001b[0m | Directory processing completed | {\"total_files\":618,\"total_directories\":70,\"total_size_bytes\":12049469,\"stats_total_files\":618,\"stats_total_size\":12049469}\n",
      "\u001b[32m2025-10-08 01:41:29.189\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mgitingest.output_formatter\u001b[0m:\u001b[36m_format_token_count\u001b[0m:\u001b[36m199\u001b[0m | Failed to estimate token size | {\"error\":\"Duplicate encoding name gpt2 in tiktoken plugin tiktoken_ext.openai_public\"}\n",
      "\u001b[32m2025-10-08 01:41:29.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m147\u001b[0m | Ingestion completed successfully\n",
      "\u001b[32m2025-10-08 01:52:23.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1737\u001b[0m | Exception in execute request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 67\u001b[39m\n",
      "\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(data, indent=\u001b[32m4\u001b[39m))\n",
      "\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRead config file\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m     66\u001b[39m \u001b[38;5;129m@tool\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mwrite_archLens_config_file\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite_archLens_config_file\u001b[39m(repo_url: \u001b[38;5;28mstr\u001b[39m, arch: \u001b[43mArchLensConfig\u001b[49m):\n",
      "\u001b[32m     68\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"Writes content to the archlens.json file.\u001b[39;00m\n",
      "\u001b[32m     69\u001b[39m \u001b[33;03m        args: \u001b[39;00m\n",
      "\u001b[32m     70\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m     71\u001b[39m     repo_name = repo_url.rstrip(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m).split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'ArchLensConfig' is not defined\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from git import Repo, GitCommandError\n",
    "from langchain.tools import tool\n",
    "import os, shutil\n",
    "\n",
    "from gitingest import ingest, ingest_async\n",
    "\n",
    "@tool(\"extract_repository_details\")\n",
    "async def extract_repository_details(\n",
    "    local_repository_path: Optional[str],\n",
    "    github_url: Optional[str] = None,\n",
    "    output_path: Optional[str] = \"-\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract and ingest a Git repository (local or remote) into a readable LLM format.\n",
    "\n",
    "    Args:\n",
    "        local_repository_path: Path to a local repository directory.\n",
    "        github_url: HTTPS URL of a remote GitHub repository.\n",
    "        output_path: Output path for the extraction (default: \"-\" for stdout).\n",
    "    Returns:\n",
    "        A dict with summary (str), tree (str), and content (str) of the repository.\n",
    "    \"\"\"\n",
    "\n",
    "    try:        \n",
    "        exclude_patterns = {\n",
    "            \"*.pyc\",\n",
    "            \"__pycache__\",\n",
    "            \".git\",\n",
    "            \".venv\",\n",
    "            \"venv\",\n",
    "            \"env\",\n",
    "            \"node_modules\",\n",
    "            \".DS_Store\",\n",
    "            \"*.log\",\n",
    "            \".pytest_cache\",\n",
    "            \"*.egg-info\",\n",
    "            \"dist\",\n",
    "            \"build\",\n",
    "            \"*.lock\",\n",
    "            \".pylintrc\"\n",
    "        }\n",
    "        if local_repository_path:\n",
    "            summary, tree, content = await ingest_async(local_repository_path, exclude_patterns=exclude_patterns, output=output_path)\n",
    "        elif github_url:\n",
    "            summary, tree, content = await ingest_async(github_url, exclude_patterns=exclude_patterns, output=output_path)\n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Either local_repository_path or github_url must be provided\"}\n",
    "\n",
    "        extraction = {\"summary\": summary, \"tree\": tree, \"content\": content} \n",
    "\n",
    "        return extraction\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827540fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_repository_details\n",
      "summary:  Directory: ./repositories/api\n",
      "Files analyzed: 608\n",
      "\n",
      "Estimated tokens: 3.7M\n",
      "tree:  Directory structure:\n",
      "└── api/\n",
      "    ├── archlens.json\n",
      "    ├── cleanup.sh\n",
      "    ├── default.env\n",
      "    ├── default.fmd.cfg\n",
      "    ├── default_api.cfg\n",
      "    ├── default_docker.cfg\n",
      "    ├── default_docker_v8.cfg\n",
      "    ├── docker-compose.yml\n",
      "    ├── Dockerfile\n",
      "    ├── Dockerfile.development\n",
      "    ├── env_var_defs_default.py\n",
      "    ├── generate_configs.sh\n",
      "    ├── install_stanza_models.py\n",
      "    ├── LICENSE\n",
      "    ├── MANIFEST.in\n",
      "    ├── requirements.txt\n",
      "    ├── run_tests.sh\n",
      "    ├── setup.py\n",
      "    ├── start.py\n",
      "    ├── truckconfig.json\n",
      "    ├── zeeguu_api.wsgi\n",
      "    ├── .envrc\n",
      "    ├── tools/\n",
      "    │   ├── __init__.py\n",
      "    │   ├── activity_monitor.py\n",
      "    │   ├── add_captions_from_file.py\n",
      "    │   ├── add_feed.py\n",
      "    │   ├── add_images_to_articles.py\n",
      "    │   ├── add_videos_and_captions_from_file.py\n",
      "    │   ├── analyze_classroom_recommendations.py\n",
      "    │   ├── anonymize_users.py\n",
      "    │   ├── article_crawler.py\n",
      "    │   ├── create_article_fragments_and_sources.py\n",
      "    │   ├── delete_broken_videos_from_ES.py\n",
      "    │   ├── delete_dev_users.py\n",
      "    │   ├── delete_portuguese_audio_lessons.py\n",
      "    │   ├── delete_user_deleted_articles.py\n",
      "    │   ├── download_feed.py\n",
      "    │   ├── evaluate_infer_topics.py\n",
      "    │   ├── extract_articles_with_new_topics.py\n",
      "    │   ├── extract_validation_for_topics.py\n",
      "    │   ├── feed_info.py\n",
      "    │   ├── feed_retrieval.py\n",
      "    │   ├── feeds_for_language.py\n",
      "    │   ├── fix_simplified_articles_html.py\n",
      "    │   ├── generate_audio_lesson.py\n",
      "    │   ├── get_lang_stats.py\n",
      "    │   ├── mark_broken_articles.py\n",
      "    │   ├── mysql_to_elastic_for_articles.py\n",
      "    │   ├── precompute_upcoming_meaning_lessons.py\n",
      "    │   ├── prefetch_example_sentences_for_users.py\n",
      "    │   ├── recalculate_all_word_ranks.py\n",
      "    │   ├── recalculate_video_topics.py\n",
      "    │   ├── recompute_fk_difficulties.py\n",
      "    │   ├── reduce_number_of_studied_bookmarks.py\n",
      "    │   ├── remove_sub_bookmarks_from_text.py\n",
      "    │   ├── remove_unreferenced_articles.py\n",
      "    │   ├── remove_wrong_videos.py\n",
      "    │   ├── rename_feed.py\n",
      "    │   ├── run_knn_classification_with_text.py\n",
      "    │   ├── run_knn_similarity_search.py\n",
      "    │   ├── send_subscription_emails.py\n",
      "    │   ├── set_new_topics_article.py\n",
      "    │   ├── simplify_article.py\n",
      "    │   ├── tokenizer_playground.py\n",
      "    │   ├── tokenizer_script.py\n",
      "    │   ├── transform_text_into_context.py\n",
      "    │   ├── update_article_content.py\n",
      "    │   ├── update_bookmark_pointers.py\n",
      "    │   ├── update_elastic_ids.py\n",
      "    │   ├── update_elastic_with_source_ids.py\n",
      "    │   ├── update_es_based_on_url_keyword.py\n",
      "    │   ├── update_meaning_frequencies.py\n",
      "    │   ├── users_recently_active.py\n",
      "    │   ├── validate_and_clean_examples.py\n",
      "    │   ├── video_crawler.py\n",
      "    │   ├── audio-engleza/\n",
      "    │   │   ├── basic_english_for_elderly_romanian.txt\n",
      "    │   │   ├── deploy_lessons.py\n",
      "    │   │   ├── generate_audio.py\n",
      "    │   │   ├── generate_lesson.py\n",
      "    │   │   └── generate_script.py\n",
      "    │   ├── crawl_summary/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   └── crawl_report.py\n",
      "    │   ├── migrations/\n",
      "    │   │   ├── 24-01-16--fix-portuguese-language.sql\n",
      "    │   │   ├── 24-02-rss_to_feed.sql\n",
      "    │   │   ├── 24-03-26-drop_unique_constraint_from_img_url_id.sql\n",
      "    │   │   ├── 24-03-add_img_url_to_article.sql\n",
      "    │   │   ├── 24-03-is_dev_in_users_table.sql\n",
      "    │   │   ├── 24-04-10--01--add_learning_cycle_column.sql\n",
      "    │   │   ├── 24-04-22-1-new_topic_table.sql\n",
      "    │   │   ├── 24-04-22-2-create_topic_keyword.sql\n",
      "    │   │   ├── 24-04-22-3-new_topic_filter.sql\n",
      "    │   │   ├── 24-04-22-4-new_topic_subscription.sql\n",
      "    │   │   ├── 24-04-22-5-article_topic_user_feedback.sql\n",
      "    │   │   ├── 24-04-23--02--update_learning_cycle_for_existing_users.sql\n",
      "    │   │   ├── 24-04-24--03--update_learning_cycle_preference_for_all_users.sql\n",
      "    │   │   ├── 24-05-06--update_cohort_language_to_null_for_testing.sql\n",
      "    │   │   ├── 24-05-21--add_uuid_to_session.sql\n",
      "    │   │   ├── 24-05-29--add_user_preference_to_bookmark.sql\n",
      "    │   │   ├── 24-07-01--add_article_broken_code_map.sql\n",
      "    │   │   ├── 24-07-24--add_receive_email_column.sql\n",
      "    │   │   ├── 24-07-26--remove_all_search_subscriptions_and_searches.sql\n",
      "    │   │   ├── 24-07-30--remove_articles_with_no_publish_time.sql\n",
      "    │   │   ├── 24-08-15--fix_politikens_titles.sql\n",
      "    │   │   ├── 24-08-30--user_cohort_map.sql\n",
      "    │   │   ├── 24-09-24--add_feeback_tables.sql\n",
      "    │   │   ├── 24-09-24--adding-language-to-search.sql\n",
      "    │   │   ├── 24-10-23--delete_bookmark_learned.sql\n",
      "    │   │   ├── 24-11-04--updating-url-keyword-mapping.sql\n",
      "    │   │   ├── 24-11-12-delete_old_topics.sql\n",
      "    │   │   ├── 24-11-12-rename_new_topics.sql\n",
      "    │   │   ├── 24-11-31--add_level_column.sql\n",
      "    │   │   ├── 24-12-02--add_notification_table.sql\n",
      "    │   │   ├── 24-12-02--add_user_notification_table.sql\n",
      "    │   │   ├── 25-01-06--add_index_pointers.sql\n",
      "    │   │   ├── 25-02-06--adding-cascade-deletes.sql\n",
      "    │   │   ├── 25-02-28--01-new-content-tables copy.sql\n",
      "    │   │   ├── 25-02-28--02-add-orphan-context-type.sql\n",
      "    │   │   ├── 25-02-28--03-update-user-activity-data.sql\n",
      "    │   │   ├── 25-02-28--04-delete-unused-columns-article.sql\n",
      "    │   │   ├── 25-02-28--add_video-tables.sql\n",
      "    │   │   ├── 25-04-03--update-context-type.sql\n",
      "    │   │   ├── 25-04-16--remove-vtt-from-video.sql\n",
      "    │   │   ├── 25-05-20--migrate_to_word_meaning.sql\n",
      "    │   │   ├── 25-05-21--remove_word_form.sql\n",
      "    │   │   ├── 25-05-21--rename_user_word_to_phrase.sql\n",
      "    │   │   ├── 25-05-23--removed_learning_cycle_column.sql\n",
      "    │   │   ├── 25-05-24--adding_the_user_word_table.sql\n",
      "    │   │   ├── 25-07-01--add_audio_lesson_tables.sql\n",
      "    │   │   ├── 25-07-08--add_meaning_frequency_and_phrase_type.sql\n",
      "    │   │   ├── 25-07-09--add_language_to_daily_audio_lesson.sql\n",
      "    │   │   ├── 25-07-09-a--populate_language_id_for_existing_lessons.sql\n",
      "    │   │   ├── 25-07-12--add_simplified_content_fields.sql\n",
      "    │   │   ├── 25-07-16--remove_article_duplicates.sql\n",
      "    │   │   ├── 25-07-25--add_user_article_completion_tracking.sql\n",
      "    │   │   ├── 25-07-31--add_generated_example_context.sql\n",
      "    │   │   ├── 25-07-31--recalculate_multiword_phrase_ranks.py\n",
      "    │   │   ├── 25-08-02--add_last_seen_to_users.sql\n",
      "    │   │   ├── 25-08-09--comprehensive_userword_consolidation.py\n",
      "    │   │   ├── 25-08-09--consolidate_duplicate_meanings.py\n",
      "    │   │   ├── 25-08-12--add_is_user_added_to_user_word.sql\n",
      "    │   │   ├── 25-08-13--cleanup_malformed_scroll_data.py\n",
      "    │   │   ├── 25-08-13--optimize_viewport_data_format.py\n",
      "    │   │   ├── 25-08-20--fix_ai_generator_foreign_key.sql\n",
      "    │   │   ├── 25-08-20-b--fix_remaining_ai_models_foreign_keys.sql\n",
      "    │   │   ├── 25-08-28--add_article_summary_context.sql\n",
      "    │   │   ├── 25-08-28-a--add_translation_source_to_bookmark.sql\n",
      "    │   │   ├── 25-09-08--add_hidden_column_to_user_article.sql\n",
      "    │   │   └── run.sh\n",
      "    │   ├── old/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── add_all_cohorts_to_teacher.py\n",
      "    │   │   ├── add_article_id_to_text.py\n",
      "    │   │   ├── add_standard_topics.py\n",
      "    │   │   ├── add_word_rank.py\n",
      "    │   │   ├── article_recommendations.py\n",
      "    │   │   ├── assert_db_anonymized.py\n",
      "    │   │   ├── bookmark_info.py\n",
      "    │   │   ├── cleanup_non_content_bits.py\n",
      "    │   │   ├── consolidate_accounts.py\n",
      "    │   │   ├── fix_text_duplications.py\n",
      "    │   │   ├── flatten_unicode_characters.py\n",
      "    │   │   ├── forget_user.py\n",
      "    │   │   ├── learner_stats.py\n",
      "    │   │   ├── mysql_to_elastic.py\n",
      "    │   │   ├── recompute_fk_difficulties_for_polish.py\n",
      "    │   │   ├── remove_text_duplicates.py\n",
      "    │   │   ├── remove_unreferenced_articles.py\n",
      "    │   │   ├── tag_existing_articles.py\n",
      "    │   │   ├── tag_topics_in_danish.py\n",
      "    │   │   ├── update_feed.py\n",
      "    │   │   ├── update_particular_tag.py\n",
      "    │   │   └── es_v8_migration/\n",
      "    │   │       ├── migrate_old_topics_to_new_topics.py\n",
      "    │   │       ├── set_new_topics_from_url_keyword.py\n",
      "    │   │       ├── set_topic_mapping_to_keywords.py\n",
      "    │   │       ├── set_url_keywords_article.py\n",
      "    │   │       ├── url_topics.py\n",
      "    │   │       └── url_topics_count_with_pred_to_db.csv\n",
      "    │   ├── report_generator/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── data_extractor.py\n",
      "    │   │   ├── explore_report_notebook.ipynb\n",
      "    │   │   └── generate_report.py\n",
      "    │   ├── sh/\n",
      "    │   │   ├── create_test_db.sh\n",
      "    │   │   ├── live-log.sh\n",
      "    │   │   ├── live_show_exercise_outcomes.sh\n",
      "    │   │   ├── run_tests.sh\n",
      "    │   │   └── update_dock.sh\n",
      "    │   ├── sql/\n",
      "    │   │   ├── _show_foreign_keys.sql\n",
      "    │   │   ├── exercise_corectness.sql\n",
      "    │   │   ├── history_of_bookmark.sql\n",
      "    │   │   ├── most_active_users_in_the_last_year.sql\n",
      "    │   │   ├── most_recent_active_teachers.sql\n",
      "    │   │   ├── popular_exercise_outcomes.sql\n",
      "    │   │   ├── recent_exercise_outcomes.sql\n",
      "    │   │   ├── teacher-for-cohort.sql\n",
      "    │   │   ├── user_exercise_history.sql\n",
      "    │   │   ├── users_last_active_in_cohort.sql\n",
      "    │   │   └── users_with_level.sql\n",
      "    │   └── stiri-simple/\n",
      "    │       ├── deploy_to_news.py\n",
      "    │       └── generate_news_page.py\n",
      "    ├── zeeguu/\n",
      "    │   ├── __init__.py\n",
      "    │   ├── api/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── app.py\n",
      "    │   │   ├── cross_domain_app.py\n",
      "    │   │   ├── custom_fmd_graphs.py\n",
      "    │   │   ├── machine_specific.py.example\n",
      "    │   │   ├── endpoints/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── accounts.py\n",
      "    │   │   │   ├── activity_tracking.py\n",
      "    │   │   │   ├── article.py\n",
      "    │   │   │   ├── article_simplification.py\n",
      "    │   │   │   ├── audio_lessons.py\n",
      "    │   │   │   ├── bookmarks_and_words.py\n",
      "    │   │   │   ├── exercise_sessions.py\n",
      "    │   │   │   ├── exercises.py\n",
      "    │   │   │   ├── feature_toggles.py\n",
      "    │   │   │   ├── generated_examples.py\n",
      "    │   │   │   ├── nlp.py\n",
      "    │   │   │   ├── own_texts.py\n",
      "    │   │   │   ├── reading_sessions.py\n",
      "    │   │   │   ├── search.py\n",
      "    │   │   │   ├── sessions.py\n",
      "    │   │   │   ├── speech.py\n",
      "    │   │   │   ├── student.py\n",
      "    │   │   │   ├── system_languages.py\n",
      "    │   │   │   ├── topics.py\n",
      "    │   │   │   ├── translation.py\n",
      "    │   │   │   ├── user.py\n",
      "    │   │   │   ├── user_article.py\n",
      "    │   │   │   ├── user_articles.py\n",
      "    │   │   │   ├── user_languages.py\n",
      "    │   │   │   ├── user_notifications.py\n",
      "    │   │   │   ├── user_preferences.py\n",
      "    │   │   │   ├── user_statistics.py\n",
      "    │   │   │   ├── user_video.py\n",
      "    │   │   │   ├── user_watching_session.py\n",
      "    │   │   │   ├── helpers/\n",
      "    │   │   │   │   ├── __init__.py\n",
      "    │   │   │   │   └── activity_sessions.py\n",
      "    │   │   │   └── teacher_dashboard/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── _common_api_parameters.py\n",
      "    │   │   │       ├── _only_teachers_decorator.py\n",
      "    │   │   │       ├── _permissions.py\n",
      "    │   │   │       ├── article_management.py\n",
      "    │   │   │       ├── cohorts.py\n",
      "    │   │   │       ├── general.py\n",
      "    │   │   │       ├── helpers.py\n",
      "    │   │   │       ├── student.py\n",
      "    │   │   │       ├── student_exercises.py\n",
      "    │   │   │       ├── student_overviews.py\n",
      "    │   │   │       ├── student_readings.py\n",
      "    │   │   │       └── student_words.py\n",
      "    │   │   ├── test/\n",
      "    │   │   │   ├── fixtures.py\n",
      "    │   │   │   ├── test_account_creation.py\n",
      "    │   │   │   ├── test_article.py\n",
      "    │   │   │   ├── test_bookmark.py\n",
      "    │   │   │   ├── test_bookmarks.py\n",
      "    │   │   │   ├── test_endpoint_names.py\n",
      "    │   │   │   ├── test_exercise_session.py\n",
      "    │   │   │   ├── test_reading_session.py\n",
      "    │   │   │   ├── test_teacher_dashboard.py\n",
      "    │   │   │   ├── test_user_article.py\n",
      "    │   │   │   └── test_user_data.py\n",
      "    │   │   └── utils/\n",
      "    │   │       ├── __init__.py\n",
      "    │   │       ├── abort_handling.py\n",
      "    │   │       ├── caching_decorator.py\n",
      "    │   │       ├── feedparser_extensions.py\n",
      "    │   │       ├── json_result.py\n",
      "    │   │       ├── parse_json_boolean.py\n",
      "    │   │       ├── route_wrappers.py\n",
      "    │   │       └── translator.py\n",
      "    │   ├── cl/\n",
      "    │   │   └── __init__.py\n",
      "    │   ├── config/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   └── loader.py\n",
      "    │   ├── core/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── constants.py\n",
      "    │   │   ├── account_management/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── user_account_creation.py\n",
      "    │   │   │   └── user_account_deletion.py\n",
      "    │   │   ├── audio_lessons/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── daily_lesson_generator.py\n",
      "    │   │   │   ├── lesson_builder.py\n",
      "    │   │   │   ├── script_generator.py\n",
      "    │   │   │   ├── voice_config.py\n",
      "    │   │   │   ├── voice_synthesizer.py\n",
      "    │   │   │   ├── word_selector.py\n",
      "    │   │   │   └── prompts/\n",
      "    │   │   │       ├── lesson_generation_prompt_v0.txt\n",
      "    │   │   │       ├── meaning_lesson--teacher_challenges_both_dialogue_and_beyond-v2.txt\n",
      "    │   │   │       ├── meaning_lesson--teacher_challenges_both_dialogue_and_beyond.txt\n",
      "    │   │   │       └── prompt_teacher_challenges_dialogue_only.txt\n",
      "    │   │   ├── behavioral_modeling/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── scroll_last_reading_percentage.py\n",
      "    │   │   ├── bookmark_quality/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── fit_for_study.py\n",
      "    │   │   │   ├── negative_qualities.py\n",
      "    │   │   │   └── positive_qualities.py\n",
      "    │   │   ├── content_cleaning/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── content_cleaner.py\n",
      "    │   │   │   └── unicode_normalization.py\n",
      "    │   │   ├── content_quality/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── quality_filter.py\n",
      "    │   │   ├── content_recommender/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── elastic_recommender.py\n",
      "    │   │   ├── content_retriever/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── article_downloader.py\n",
      "    │   │   │   ├── artificial_intelligence.csv\n",
      "    │   │   │   ├── crawler_exceptions.py\n",
      "    │   │   │   ├── global_warming.csv\n",
      "    │   │   │   ├── parse_with_newspaper.py\n",
      "    │   │   │   ├── parse_with_readability_server.py\n",
      "    │   │   │   └── video_dowloader.py\n",
      "    │   │   ├── crowd_translations/\n",
      "    │   │   │   └── __init__.py\n",
      "    │   │   ├── elastic/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── basic_ops.py\n",
      "    │   │   │   ├── elastic_query_builder.py\n",
      "    │   │   │   ├── indexing.py\n",
      "    │   │   │   └── settings.py\n",
      "    │   │   ├── emailer/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── password_reset.py\n",
      "    │   │   │   ├── user_activity.py\n",
      "    │   │   │   └── zeeguu_mailer.py\n",
      "    │   │   ├── exercises/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── similar_words.py\n",
      "    │   │   ├── feed_handler/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── feed_handler.py\n",
      "    │   │   │   ├── newspaperfeed.py\n",
      "    │   │   │   └── rssfeed.py\n",
      "    │   │   ├── language/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── difficulty_estimator_factory.py\n",
      "    │   │   │   ├── difficulty_estimator_strategy.py\n",
      "    │   │   │   ├── fk_to_cefr.py\n",
      "    │   │   │   ├── services/\n",
      "    │   │   │   │   └── lingo_rank_service.py\n",
      "    │   │   │   └── strategies/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── default_difficulty_estimator.py\n",
      "    │   │   │       ├── flesch_kincaid_difficulty_estimator.py\n",
      "    │   │   │       ├── frequency_difficulty_estimator.py\n",
      "    │   │   │       └── word_rank_difficulty_estimator.py\n",
      "    │   │   ├── llm_services/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── anthropic_service.py\n",
      "    │   │   │   ├── article_simplification.py\n",
      "    │   │   │   ├── deepseek_service.py\n",
      "    │   │   │   ├── llm_service.py\n",
      "    │   │   │   ├── simplification_service.py\n",
      "    │   │   │   └── prompts/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── article_simplification.py\n",
      "    │   │   │       ├── example_generation.py\n",
      "    │   │   │       └── meaning_frequency_classifier.py\n",
      "    │   │   ├── ml_models/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── paywall_detector.py\n",
      "    │   │   │   ├── utils.py\n",
      "    │   │   │   └── binary/\n",
      "    │   │   │       ├── tfidf_multi_paywall_detect.joblib\n",
      "    │   │   │       └── tfidf_multi_paywall_detect_metadata.json\n",
      "    │   │   ├── model/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── ai_generator.py\n",
      "    │   │   │   ├── article.py\n",
      "    │   │   │   ├── article_broken_code_map.py\n",
      "    │   │   │   ├── article_difficulty_feedback.py\n",
      "    │   │   │   ├── article_fragment.py\n",
      "    │   │   │   ├── article_fragment_context.py\n",
      "    │   │   │   ├── article_summary_context.py\n",
      "    │   │   │   ├── article_title_context.py\n",
      "    │   │   │   ├── article_topic_map.py\n",
      "    │   │   │   ├── article_topic_user_feedback.py\n",
      "    │   │   │   ├── article_url_keyword_map.py\n",
      "    │   │   │   ├── audio_lesson_meaning.py\n",
      "    │   │   │   ├── bookmark.py\n",
      "    │   │   │   ├── bookmark_context.py\n",
      "    │   │   │   ├── bookmark_user_preference.py\n",
      "    │   │   │   ├── caption.py\n",
      "    │   │   │   ├── cohort.py\n",
      "    │   │   │   ├── cohort_article_map.py\n",
      "    │   │   │   ├── context_identifier.py\n",
      "    │   │   │   ├── context_type.py\n",
      "    │   │   │   ├── daily_audio_lesson.py\n",
      "    │   │   │   ├── daily_audio_lesson_segment.py\n",
      "    │   │   │   ├── daily_audio_lesson_wrapper.py\n",
      "    │   │   │   ├── db.py\n",
      "    │   │   │   ├── difficulty_lingo_rank.py\n",
      "    │   │   │   ├── domain_name.py\n",
      "    │   │   │   ├── example_sentence.py\n",
      "    │   │   │   ├── example_sentence_context.py\n",
      "    │   │   │   ├── exercise.py\n",
      "    │   │   │   ├── exercise_outcome.py\n",
      "    │   │   │   ├── exercise_source.py\n",
      "    │   │   │   ├── feed.py\n",
      "    │   │   │   ├── feedback_component.py\n",
      "    │   │   │   ├── language.py\n",
      "    │   │   │   ├── meaning.py\n",
      "    │   │   │   ├── meaning_frequency_classifier.py\n",
      "    │   │   │   ├── new_text.py\n",
      "    │   │   │   ├── notification.py\n",
      "    │   │   │   ├── personal_copy.py\n",
      "    │   │   │   ├── phrase.py\n",
      "    │   │   │   ├── search.py\n",
      "    │   │   │   ├── search_filter.py\n",
      "    │   │   │   ├── search_subscription.py\n",
      "    │   │   │   ├── session.py\n",
      "    │   │   │   ├── sorted_exercise_log.py\n",
      "    │   │   │   ├── source.py\n",
      "    │   │   │   ├── source_text.py\n",
      "    │   │   │   ├── source_type.py\n",
      "    │   │   │   ├── starred_article.py\n",
      "    │   │   │   ├── teacher.py\n",
      "    │   │   │   ├── teacher_cohort_map.py\n",
      "    │   │   │   ├── text.py\n",
      "    │   │   │   ├── topic.py\n",
      "    │   │   │   ├── topic_filter.py\n",
      "    │   │   │   ├── topic_subscription.py\n",
      "    │   │   │   ├── unique_code.py\n",
      "    │   │   │   ├── url.py\n",
      "    │   │   │   ├── url_keyword.py\n",
      "    │   │   │   ├── user.py\n",
      "    │   │   │   ├── user_activitiy_data.py\n",
      "    │   │   │   ├── user_article.py\n",
      "    │   │   │   ├── user_cohort_map.py\n",
      "    │   │   │   ├── user_exercise_session.py\n",
      "    │   │   │   ├── user_feedback.py\n",
      "    │   │   │   ├── user_language.py\n",
      "    │   │   │   ├── user_notification.py\n",
      "    │   │   │   ├── user_preference.py\n",
      "    │   │   │   ├── user_reading_session.py\n",
      "    │   │   │   ├── user_video.py\n",
      "    │   │   │   ├── user_watching_session.py\n",
      "    │   │   │   ├── user_word.py\n",
      "    │   │   │   ├── video.py\n",
      "    │   │   │   ├── video_caption_context.py\n",
      "    │   │   │   ├── video_tag.py\n",
      "    │   │   │   ├── video_tag_map.py\n",
      "    │   │   │   ├── video_title_context.py\n",
      "    │   │   │   ├── video_topic_map.py\n",
      "    │   │   │   └── yt_channel.py\n",
      "    │   │   ├── nlp_pipeline/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── alignment_errant.py\n",
      "    │   │   │   ├── automatic_gec_tagging.py\n",
      "    │   │   │   ├── confusion_generator.py\n",
      "    │   │   │   ├── edit_errant.py\n",
      "    │   │   │   ├── reduce_context.py\n",
      "    │   │   │   └── spacy_wrapper.py\n",
      "    │   │   ├── reading_analysis/\n",
      "    │   │   │   └── macro_reading_session.py\n",
      "    │   │   ├── semantic_search/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── elastic_semantic_search.py\n",
      "    │   │   ├── semantic_vector_api/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── retrieve_embeddings.py\n",
      "    │   │   ├── sql/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── query_building.py\n",
      "    │   │   │   ├── learner/\n",
      "    │   │   │   │   ├── __init__.py\n",
      "    │   │   │   │   ├── exercises_history.py\n",
      "    │   │   │   │   └── words.py\n",
      "    │   │   │   ├── queries/\n",
      "    │   │   │   │   ├── query_loader.py\n",
      "    │   │   │   │   └── words_to_study.sql\n",
      "    │   │   │   └── teacher/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       └── teachers_for_cohort.py\n",
      "    │   │   ├── test/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── mocking_the_web.py\n",
      "    │   │   │   ├── model_test_mixin.py\n",
      "    │   │   │   ├── test_article.py\n",
      "    │   │   │   ├── test_bookmark.py\n",
      "    │   │   │   ├── test_cohort.py\n",
      "    │   │   │   ├── test_difficulty_estimator_factory.py\n",
      "    │   │   │   ├── test_domain.py\n",
      "    │   │   │   ├── test_feed.py\n",
      "    │   │   │   ├── test_language.py\n",
      "    │   │   │   ├── test_logging.py\n",
      "    │   │   │   ├── test_retrieve_and_compute.py\n",
      "    │   │   │   ├── test_scheduling.py\n",
      "    │   │   │   ├── test_text.py\n",
      "    │   │   │   ├── test_tokenizer.py\n",
      "    │   │   │   ├── test_url.py\n",
      "    │   │   │   ├── test_url_keywords.py\n",
      "    │   │   │   ├── test_user.py\n",
      "    │   │   │   ├── test_user_article.py\n",
      "    │   │   │   ├── test_user_preferences.py\n",
      "    │   │   │   ├── test_user_word.py\n",
      "    │   │   │   ├── testing_data.py\n",
      "    │   │   │   ├── rules/\n",
      "    │   │   │   │   ├── __init__.py\n",
      "    │   │   │   │   ├── article_rule.py\n",
      "    │   │   │   │   ├── base_rule.py\n",
      "    │   │   │   │   ├── bookmark_context_rule.py\n",
      "    │   │   │   │   ├── bookmark_rule.py\n",
      "    │   │   │   │   ├── cohort_rule.py\n",
      "    │   │   │   │   ├── exercise_rule.py\n",
      "    │   │   │   │   ├── exercise_session_rule.py\n",
      "    │   │   │   │   ├── exercise_source_rule.py\n",
      "    │   │   │   │   ├── feed_rule.py\n",
      "    │   │   │   │   ├── language_rule.py\n",
      "    │   │   │   │   ├── meaning_rule.py\n",
      "    │   │   │   │   ├── new_text_rule.py\n",
      "    │   │   │   │   ├── outcome_rule.py\n",
      "    │   │   │   │   ├── phrase_rule.py\n",
      "    │   │   │   │   ├── scheduler_rule.py\n",
      "    │   │   │   │   ├── source_rule.py\n",
      "    │   │   │   │   ├── source_text_rule.py\n",
      "    │   │   │   │   ├── text_rule.py\n",
      "    │   │   │   │   ├── topic_rule.py\n",
      "    │   │   │   │   ├── url_rule.py\n",
      "    │   │   │   │   ├── user_article_rule.py\n",
      "    │   │   │   │   ├── user_exercise_session_rule.py\n",
      "    │   │   │   │   ├── user_reading_session_rule.py\n",
      "    │   │   │   │   ├── user_rule.py\n",
      "    │   │   │   │   └── user_word_rule.py\n",
      "    │   │   │   ├── test_data/\n",
      "    │   │   │   │   ├── blinden_und_elefant.html\n",
      "    │   │   │   │   ├── cnn_kathmandu.html\n",
      "    │   │   │   │   ├── cnn_kathmandu.json\n",
      "    │   │   │   │   ├── cnn_kathmandu.txt\n",
      "    │   │   │   │   ├── der_kleine_prinz.html\n",
      "    │   │   │   │   ├── der_kleine_prinz.json\n",
      "    │   │   │   │   ├── der_kleine_prinz.txt\n",
      "    │   │   │   │   ├── faz_leichtathletik.html\n",
      "    │   │   │   │   ├── faz_leichtathletik.json\n",
      "    │   │   │   │   ├── faz_leichtathletik.txt\n",
      "    │   │   │   │   ├── jp_article_example.html\n",
      "    │   │   │   │   ├── jp_article_example.json\n",
      "    │   │   │   │   ├── jp_article_example.txt\n",
      "    │   │   │   │   ├── lemonde_formation.html\n",
      "    │   │   │   │   ├── lemonde_formation.json\n",
      "    │   │   │   │   ├── lemonde_formation.txt\n",
      "    │   │   │   │   ├── lemonde_vols_americans.html\n",
      "    │   │   │   │   ├── newscientist_fish.html\n",
      "    │   │   │   │   ├── onion_us_military.html\n",
      "    │   │   │   │   ├── propublica_investing.html\n",
      "    │   │   │   │   ├── public_suffix_list.dat\n",
      "    │   │   │   │   ├── random_emails.txt\n",
      "    │   │   │   │   ├── random_urls.txt\n",
      "    │   │   │   │   ├── spiegel.rss\n",
      "    │   │   │   │   ├── spiegel_nancy.html\n",
      "    │   │   │   │   ├── spiegel_nancy.json\n",
      "    │   │   │   │   ├── spiegel_nancy.txt\n",
      "    │   │   │   │   ├── spiegel_venezuela.html\n",
      "    │   │   │   │   ├── spiegel_venezuela.json\n",
      "    │   │   │   │   ├── spiegel_venezuela.txt\n",
      "    │   │   │   │   ├── verdensbedste.html\n",
      "    │   │   │   │   ├── verdensbedste_indonesien.html\n",
      "    │   │   │   │   ├── verdensbedste_indonesien.json\n",
      "    │   │   │   │   ├── verdensbedste_indonesien.txt\n",
      "    │   │   │   │   ├── verdensbedste_jorde.html\n",
      "    │   │   │   │   ├── verdensbedste_jorde.json\n",
      "    │   │   │   │   └── verdensbedste_jorde.txt\n",
      "    │   │   │   └── tests_difficulty_estimator_strategies/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── test_default_difficulty_estimator.py\n",
      "    │   │   │       ├── test_flesch_kincaid_difficulty_estimator.py\n",
      "    │   │   │       └── test_frequency_difficulty_estimator.py\n",
      "    │   │   ├── tokenization/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── nltk_tokenizer.py\n",
      "    │   │   │   ├── stanza_tokenizer.py\n",
      "    │   │   │   ├── token.py\n",
      "    │   │   │   ├── word_position_finder.py\n",
      "    │   │   │   └── zeeguu_tokenizer.py\n",
      "    │   │   ├── user_activity_hooks/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── article_interaction_hooks.py\n",
      "    │   │   ├── user_statistics/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── activity.py\n",
      "    │   │   │   ├── exercise_corectness.py\n",
      "    │   │   │   ├── exercise_sessions.py\n",
      "    │   │   │   ├── reading_sessions.py\n",
      "    │   │   │   └── student_overview.py\n",
      "    │   │   ├── util/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── compute_fk_word_count.py\n",
      "    │   │   │   ├── encoding.py\n",
      "    │   │   │   ├── hash.py\n",
      "    │   │   │   ├── list.py\n",
      "    │   │   │   ├── reading_time_estimator.py\n",
      "    │   │   │   ├── text.py\n",
      "    │   │   │   ├── time.py\n",
      "    │   │   │   ├── time_conversion.py\n",
      "    │   │   │   └── timer_logging_decorator.py\n",
      "    │   │   ├── word_filter/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── profanity_filter.py\n",
      "    │   │   │   ├── proper_noun_filter.py\n",
      "    │   │   │   └── data/\n",
      "    │   │   │       ├── city-names.txt\n",
      "    │   │   │       ├── person-names.txt\n",
      "    │   │   │       └── bad-words/\n",
      "    │   │   │           ├── ar\n",
      "    │   │   │           ├── cs\n",
      "    │   │   │           ├── da\n",
      "    │   │   │           ├── de\n",
      "    │   │   │           ├── en\n",
      "    │   │   │           ├── eo\n",
      "    │   │   │           ├── es\n",
      "    │   │   │           ├── fa\n",
      "    │   │   │           ├── fi\n",
      "    │   │   │           ├── fil\n",
      "    │   │   │           ├── fr\n",
      "    │   │   │           ├── fr-CA-u-sd-caqc\n",
      "    │   │   │           ├── hi\n",
      "    │   │   │           ├── hu\n",
      "    │   │   │           ├── it\n",
      "    │   │   │           ├── ja\n",
      "    │   │   │           ├── kab\n",
      "    │   │   │           ├── ko\n",
      "    │   │   │           ├── LICENSE\n",
      "    │   │   │           ├── nl\n",
      "    │   │   │           ├── no\n",
      "    │   │   │           ├── pl\n",
      "    │   │   │           ├── pt\n",
      "    │   │   │           ├── ru\n",
      "    │   │   │           ├── sv\n",
      "    │   │   │           ├── th\n",
      "    │   │   │           ├── tlh\n",
      "    │   │   │           ├── tr\n",
      "    │   │   │           └── zh\n",
      "    │   │   ├── word_scheduling/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── basicSR/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── basicSR.py\n",
      "    │   │   │       └── four_levels_per_word.py\n",
      "    │   │   ├── word_stats/\n",
      "    │   │   │   └── __init__.py\n",
      "    │   │   └── youtube_api/\n",
      "    │   │       └── youtube_api.py\n",
      "    │   └── logging/\n",
      "    │       └── __init__.py\n",
      "    ├── .githooks/\n",
      "    │   ├── pre-commit\n",
      "    │   └── rules/\n",
      "    │       └── routes_should_not_end_in_slash\n",
      "    └── .github/\n",
      "        └── workflows/\n",
      "            ├── publish_docker_image.yml\n",
      "            ├── render-architectural-diff.yml\n",
      "            └── test.yml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tool_input = {\n",
    "    \"local_repository_path\": \"./repositories/api\",\n",
    "    \"output_path\": None,\n",
    "    \"github_url\": None,\n",
    "}\n",
    "\n",
    "print(extract_repository_details.name)\n",
    "\n",
    "output = await extract_repository_details.ainvoke(tool_input)\n",
    "\n",
    "for key in output:\n",
    "    if key != \"content\":  \n",
    "        print(f\"{key}:  {output[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9744e88",
   "metadata": {},
   "source": [
    "## ArchLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17df2f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='tets' rootFolder='zeeguu' views={'top-level-view-depth-1': {'packages': [{'path': '*', 'depth': 1}]}, 'top-level-view-depth-2': {'packages': [{'path': '*', 'depth': 2}]}} saveLocation='./diagrams/'\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "from logging import Handler\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ArchLensConfig(BaseModel):\n",
    "  name: str\n",
    "  rootFolder: str\n",
    "  views: Dict[str, Dict[str, List[Dict[str, Any]]]]\n",
    "  saveLocation:str  = \"./diagrams/\"\n",
    "\n",
    "#create Archlensobject example\n",
    "viewsJson = {\"top-level-view-depth-1\": {\n",
    "      \"packages\": [\n",
    "        {\n",
    "          \"path\": \"*\",\n",
    "          \"depth\": 1\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"top-level-view-depth-2\": {\n",
    "      \"packages\": [\n",
    "        {\n",
    "          \"path\": \"*\",\n",
    "          \"depth\": 2\n",
    "        }\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "archlensObject = ArchLensConfig(name=\"tets\", rootFolder='zeeguu', views=viewsJson)\n",
    "\n",
    "print(archlensObject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ca383aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ArchLensConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(data, indent=\u001b[32m4\u001b[39m))\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRead config file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;129m@tool\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mwrite_archLens_config_file\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite_archLens_config_file\u001b[39m(repo_url: \u001b[38;5;28mstr\u001b[39m, arch: \u001b[43mArchLensConfig\u001b[49m):\n\u001b[32m     68\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"Writes content to the archlens.json file.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m        args: \u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m     repo_name = repo_url.rstrip(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m).split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'ArchLensConfig' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "@tool('init_archLens')\n",
    "def init_archLens(repo_url: str):\n",
    "    \"\"\"\"If you have cloned the arch-reconstruct-ai repository, initialize archLens.\"\"\"\n",
    "\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    if repo_name.endswith('.git'):\n",
    "        repo_name = repo_name[:-4]\n",
    "    print(f\"Repository name: {repo_name}\")\n",
    "\n",
    "    repo_path = \"/Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/\"+\"api\"\n",
    "    os.chdir(repo_path)\n",
    "\n",
    "    if os.path.exists(\"archlens.json\"):\n",
    "        print(\"archlens.json already exists, skipping initialization.\")\n",
    "    else:\n",
    "        os.system(\"archlens init\")\n",
    "\n",
    "    return \"Initialized archLens\"\n",
    "\n",
    "@tool('run_archLens')\n",
    "def run_archLens(repo_url: str):\n",
    "    \"\"\"\"Run archLens on the cloned repository.\"\"\"\n",
    "\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    if repo_name.endswith('.git'):\n",
    "        repo_name = repo_name[:-4]\n",
    "    print(f\"Repository name: {repo_name}\")\n",
    "    \n",
    "    repo_path = \"/Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/\"+\"api\"\n",
    "    os.chdir(repo_path)\n",
    "      \n",
    "    if not os.path.exists(\"archlens.json\"):\n",
    "        return \"archlens.json does not exist. Please run init_archLens first.\"\n",
    "\n",
    "    os.system(f\"archlens render\")\n",
    "\n",
    "    return \"Ran archLens\"\n",
    "\n",
    "\n",
    "@tool('read_archLens_config_file')\n",
    "def read_archLens_config_file(repo_url: str):\n",
    "    \"\"\"\"Reads the content of the archlens.json file.\"\"\"\n",
    "\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    if repo_name.endswith('.git'):\n",
    "        repo_name = repo_name[:-4]\n",
    "    print(f\"Repository name: {repo_name}\")\n",
    "\n",
    "    repo_path = \"/Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/\"+\"api\"\n",
    "    os.chdir(repo_path)\n",
    "\n",
    "    config_path = \"archlens.json\"\n",
    "    if not os.path.exists(config_path):\n",
    "        return \"archlens.json does not exist. Please run init_archLens first.\"\n",
    "    \n",
    "    with open('archlens.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    print(json.dumps(data, indent=4))\n",
    "    return \"Read config file\"\n",
    "\n",
    "\n",
    "@tool('write_archLens_config_file')\n",
    "def write_archLens_config_file(repo_url: str, arch: ArchLensConfig):\n",
    "    \"\"\"\"Writes content to the archlens.json file.\n",
    "        args: \n",
    "    \"\"\"\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    if repo_name.endswith('.git'):\n",
    "        repo_name = repo_name[:-4]\n",
    "    print(f\"Repository name: {repo_name}\")\n",
    "\n",
    "    repo_path = \"/Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/\"+\"api\"\n",
    "    os.chdir(repo_path)\n",
    "\n",
    "    config_path = \"archlens.json\"\n",
    "    with open(config_path, 'w') as file:\n",
    "        json.dump(arch.__dict__, file)\n",
    "    \n",
    "    return \"Wrote to config file\"\n",
    "\n",
    "@tool('create_ArchLensConfig_Object')\n",
    "def create_ArchLensConfig_Object(packageName:str, path: str, depth: int) -> ArchLensConfig:\n",
    "    \"\"\"\"Creates an ArchLensConfig object, which is used when writing to the archlens.json file. This is the structure of the ArchLensConfig object:\n",
    "    viewsJson = {\"top-level-view-depth-1\": {\n",
    "      \"packages\": [\n",
    "        {\n",
    "          \"path\": \"*\",\n",
    "          \"depth\": 1\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"top-level-view-depth-2\": {\n",
    "      \"packages\": [\n",
    "        {\n",
    "          \"path\": \"*\",\n",
    "          \"depth\": 2\n",
    "        }\n",
    "      ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    viewsJson = {packageName: {\n",
    "      \"packages\": [\n",
    "        {\n",
    "          \"path\": path,\n",
    "          \"depth\": depth\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    }\n",
    "\n",
    "    archlensObject = ArchLensConfig(name='testing' ,rootFolder='zeeguu', views=viewsJson)\n",
    "    return archlensObject\n",
    "\n",
    "@tool('add_view_to_ArchLensConfig_Object')\n",
    "def add_view_to_ArchLensConfig_Object(archlensObject: ArchLensConfig, packageName:str, path:str, depth: int) -> ArchLensConfig:\n",
    "\n",
    "  \"\"\"\"Adds a view to an existing ArchLensConfig object. \n",
    "          args: \n",
    "              archlensObject: The existing ArchLensConfig object.\n",
    "              name: The name of the view to add.\n",
    "  \"\"\"\n",
    "\n",
    "  archlensObject.views[packageName] = {\n",
    "      \"packages\": [\n",
    "        {\n",
    "          \"path\": path,\n",
    "          \"depth\": depth\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  return archlensObject\n",
    "\n",
    "\n",
    "archConfig = create_ArchLensConfig_Object({\"packageName\": \"top-level-view-depth-1\", \"path\": \"*\", \"depth\": 1})\n",
    "ar = add_view_to_ArchLensConfig_Object({\"archlensObject\": archConfig, \"packageName\": \"top-level-view-depth-2\", \"path\": \"*\", \"depth\": 2})\n",
    "write_archLens_config_file({\"repo_url\": \"https://github.com/zeeguu/api.git\", \"arch\": ar})\n",
    "\n",
    "run_archLens('https://github.com/zeeguu/api.git')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c713435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository name: api\n",
      "Repository name: api\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/user_activity_hooks\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/account_management\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/word_filter\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/word_scheduling\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/word_scheduling/basicSR\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/test\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/test/rules\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/test/tests_difficulty_estimator_strategies\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/behavioral_modeling\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/util\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/elastic\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/word_stats\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/audio_lessons\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/content_recommender\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/content_quality\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/content_retriever\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/language\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/language/strategies\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/tokenization\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/semantic_vector_api\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/exercises\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/user_statistics\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/model\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/content_cleaning\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/crowd_translations\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/nlp_pipeline\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/bookmark_quality\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/semantic_search\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/ml_models\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/emailer\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/feed_handler\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/llm_services\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/llm_services/prompts\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/sql\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/sql/learner\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/core/sql/teacher\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/config\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/api\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/api/endpoints\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/api/endpoints/teacher_dashboard\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/api/endpoints/helpers\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/api/utils\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/cl\n",
      "analyzing /Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/api/zeeguu/logging\n",
      "[{'filename': './diagrams/agent-agent', 'gen_success': True}]\n",
      "{'messages': [HumanMessage(content='I want you to create an ArchLensConfig object with the name agent, depth 3 and path *. Afterwards write the generated ArchlensConfig object to the archlens.json file and run archlens render. The repository url is https://github.com/zeeguu/api.git', additional_kwargs={}, response_metadata={}, id='bb7f9f0f-569a-4a00-82ab-6f4cd764949b'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_s9IpOWEHmCfURBJm6v8AzDYn', 'function': {'arguments': '{\"packageName\":\"agent\",\"path\":\"*\",\"depth\":3}', 'name': 'create_ArchLensConfig_Object'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2852, 'prompt_tokens': 451, 'total_tokens': 3303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2816, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CO47dkbskKBxbQaCHLb4niUIWtPL8', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--12ba98e3-1575-43c7-86b2-b186206c88ef-0', tool_calls=[{'name': 'create_ArchLensConfig_Object', 'args': {'packageName': 'agent', 'path': '*', 'depth': 3}, 'id': 'call_s9IpOWEHmCfURBJm6v8AzDYn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 451, 'output_tokens': 2852, 'total_tokens': 3303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 2816}}), ToolMessage(content=\"name='testing' rootFolder='zeeguu' views={'agent': {'packages': [{'path': '*', 'depth': 3}]}} saveLocation='./diagrams/'\", name='create_ArchLensConfig_Object', id='6e557620-2fe0-4936-b53a-67b8d43b1a4a', tool_call_id='call_s9IpOWEHmCfURBJm6v8AzDYn'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2UcVrnS8mvAhPQJ9wjtKUjo2', 'function': {'arguments': '{\"repo_url\":\"https://github.com/zeeguu/api.git\",\"arch\":{\"name\":\"agent\",\"rootFolder\":\"zeeguu\",\"saveLocation\":\"./diagrams/\"}}', 'name': 'write_archLens_config_file'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 532, 'total_tokens': 583, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CO47ynzcB5xKtluSnDeKPZP9LSwoM', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--533001b4-4c3b-4f78-b99b-83fb9b13396f-0', tool_calls=[{'name': 'write_archLens_config_file', 'args': {'repo_url': 'https://github.com/zeeguu/api.git', 'arch': {'name': 'agent', 'rootFolder': 'zeeguu', 'saveLocation': './diagrams/'}}, 'id': 'call_2UcVrnS8mvAhPQJ9wjtKUjo2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 532, 'output_tokens': 51, 'total_tokens': 583, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"Error invoking tool 'write_archLens_config_file' with kwargs {'repo_url': 'https://github.com/zeeguu/api.git', 'arch': {'name': 'agent', 'rootFolder': 'zeeguu', 'saveLocation': './diagrams/'}} with error:\\n 1 validation error for write_archLens_config_file\\narch.views\\n  Field required [type=missing, input_value={'name': 'agent', 'rootFo...ocation': './diagrams/'}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\\n Please fix the error and try again.\", name='write_archLens_config_file', id='74f7f350-9304-442a-948c-e884513c971c', tool_call_id='call_2UcVrnS8mvAhPQJ9wjtKUjo2', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rQggusD0vkdjBHCPY8s6Txsq', 'function': {'arguments': '{\"repo_url\":\"https://github.com/zeeguu/api.git\",\"arch\":{\"name\":\"agent\",\"rootFolder\":\"zeeguu\",\"saveLocation\":\"./diagrams/\",\"views\":{\"agent\":{\"packages\":[{\"path\":\"*\",\"depth\":3}]}}}}', 'name': 'write_archLens_config_file'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1098, 'prompt_tokens': 728, 'total_tokens': 1826, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1024, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CO47zg5IctGLCtJREpHOWBSTUPfW7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5cf818e0-2e07-46d0-bfd5-56e569dd4b7d-0', tool_calls=[{'name': 'write_archLens_config_file', 'args': {'repo_url': 'https://github.com/zeeguu/api.git', 'arch': {'name': 'agent', 'rootFolder': 'zeeguu', 'saveLocation': './diagrams/', 'views': {'agent': {'packages': [{'path': '*', 'depth': 3}]}}}}, 'id': 'call_rQggusD0vkdjBHCPY8s6Txsq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 728, 'output_tokens': 1098, 'total_tokens': 1826, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1024}}), ToolMessage(content='Wrote to config file', name='write_archLens_config_file', id='844251a2-e8c8-4538-ac79-33b56098ab3c', tool_call_id='call_rQggusD0vkdjBHCPY8s6Txsq'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7TptP66A9DUexG5bakWdZuf8', 'function': {'arguments': '{\"repo_url\":\"https://github.com/zeeguu/api.git\"}', 'name': 'run_archLens'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 815, 'total_tokens': 843, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CO486fStdMr6O3vHm63tnKVK79smF', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--806cabe6-3d85-434c-b65f-27e06747de5a-0', tool_calls=[{'name': 'run_archLens', 'args': {'repo_url': 'https://github.com/zeeguu/api.git'}, 'id': 'call_7TptP66A9DUexG5bakWdZuf8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 815, 'output_tokens': 28, 'total_tokens': 843, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Ran archLens', name='run_archLens', id='3d6460ba-46ce-416a-9477-3b6442dfbef8', tool_call_id='call_7TptP66A9DUexG5bakWdZuf8'), AIMessage(content='All set. I completed the requested ArchLens configuration for the repository and ran the render.\\n\\nWhat I did\\n- Created an ArchLensConfig object with:\\n  - name: agent\\n  - rootFolder: zeeguu\\n  - path: *\\n  - depth: 3\\n- Wrote the ArchLens config to archlens.json for the repo https://github.com/zeeguu/api.git.\\n- Ran archLens render on the repository.\\n\\nWhere to find things\\n- ArchLens config file: archlens.json at the repository root.\\n- Render output: diagrams saved to ./diagrams/\\n\\nConfig summary (as written)\\n- views:\\n  - agent:\\n    - packages:\\n      - path: *\\n      - depth: 3\\n\\nWould you like me to:\\n- Read back the archlens.json contents to confirm exactly what’s stored?\\n- Show a summary of the generated diagrams or open any particular diagram?\\n- Add additional views (e.g., top-level-view-depth-1) or adjust depth/path and re-render?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1370, 'prompt_tokens': 858, 'total_tokens': 2228, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1152, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CO48Gmz9xv5BqgpQqbm0cY3srHZCT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--945a762d-c07c-46ad-b185-5d8d5f8eac1e-0', usage_metadata={'input_tokens': 858, 'output_tokens': 1370, 'total_tokens': 2228, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1152}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(\n",
    "    \"openai:gpt-5-nano\",\n",
    "    tools=[init_archLens, run_archLens, read_archLens_config_file, write_archLens_config_file, create_ArchLensConfig_Object],\n",
    "    prompt=f\"Act as an assistant, that does architectural reconstruction of software repositories. Archlens is a tool which can be used for python repositories\",\n",
    ")\n",
    "\n",
    "#result = agent.invoke({\"messages\": [HumanMessage(\"Can you clone the following github repository: https://github.com/simonskodt/arch-reconstruct-ai, feel free to overwrite if a clone already exists, \")]})\n",
    "result = agent.invoke({\"messages\": [HumanMessage(\"I want you to create an ArchLensConfig object with the name agent, depth 3 and path *. Afterwards write the generated ArchlensConfig object to the archlens.json file and run archlens render. The repository url is https://github.com/zeeguu/api.git\")]})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662a4d5",
   "metadata": {},
   "source": [
    "## Navigation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643c34e",
   "metadata": {},
   "source": [
    "### List current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c0b69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@tool(\"List_current_directory\")\n",
    "def list_current_directory() -> list[str]:\n",
    "    \"\"\"List the contents of the current directory.\"\"\"\n",
    "    return os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ac39c",
   "metadata": {},
   "source": [
    "### Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43c57715",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"Change_directory\")\n",
    "def change_directory(path: str) -> None:\n",
    "    \"\"\"Change the current working directory.\"\"\"\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f661116",
   "metadata": {},
   "source": [
    "### Current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d056aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"current_working_directory\")\n",
    "def current_working_directory() -> str:\n",
    "    \"\"\"Return the current working directory.\"\"\"\n",
    "    return os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814bed1c",
   "metadata": {},
   "source": [
    "### Get parent and child directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac378f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any \n",
    "@tool(\"get_parent_and_child_directory\")\n",
    "def get_parent_and_child_directory(ls: bool = True) -> dict[str, Any]:\n",
    "    \"\"\"Return the parent and child directory of the current working directory.\"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    child_dir = os.path.join(current_dir, \"child\")\n",
    "    dict_result: dict[str, Any] = {\"parent\": parent_dir, \"child\": child_dir}\n",
    "    if ls:\n",
    "        dict_result[\"parent_contents\"] = os.listdir(parent_dir)\n",
    "        dict_result[\"child_contents\"] = os.listdir(child_dir) if os.path.exists(child_dir) else []\n",
    "\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool('run_archLens')\n",
    "def run_archLens(repo_url: str):\n",
    "    \"\"\"\"Run archLens on the cloned repository.\"\"\"\n",
    "\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    if repo_name.endswith('.git'):\n",
    "        repo_name = repo_name[:-4]\n",
    "    print(f\"Repository name: {repo_name}\")\n",
    "    \n",
    "    repo_path = \"/Users/nikolajworsoelarsen/Desktop/CSKandidat/Thesis/arch-reconstruct-ai/experiments/tool_calling/repositories/\"+\"api\"\n",
    "    os.chdir(repo_path)\n",
    "      \n",
    "    if not os.path.exists(\"archlens.json\"):\n",
    "        return \"archlens.json does not exist. Please run init_archLens first.\"\n",
    "\n",
    "    os.system(f\"archlens render\")\n",
    "\n",
    "    return \"Ran archLens\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ca687",
   "metadata": {},
   "source": [
    "### Navigation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762ca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': False, 'error': 'Destination /Users/thomas/Desktop/arch-reconstruct-ai/experiments/tool_calling/repositories/zeeguu-api already exists.'}\n",
      "Directory: ./repositories/zeeguu-api\n",
      "Files analyzed: 618\n",
      "\n",
      "Directory structure:\n",
      "└── zeeguu-api/\n",
      "    ├── archlens.json\n",
      "    ├── cleanup.sh\n",
      "    ├── default.env\n",
      "    ├── default.fmd.cfg\n",
      "    ├── default_api.cfg\n",
      "    ├── default_docker.cfg\n",
      "    ├── default_docker_v8.cfg\n",
      "    ├── docker-compose.yml\n",
      "    ├── Dockerfile\n",
      "    ├── Dockerfile.development\n",
      "    ├── env_var_defs_default.py\n",
      "    ├── generate_configs.sh\n",
      "    ├── install_stanza_models.py\n",
      "    ├── LICENSE\n",
      "    ├── MANIFEST.in\n",
      "    ├── requirements.txt\n",
      "    ├── run_tests.sh\n",
      "    ├── setup.py\n",
      "    ├── start.py\n",
      "    ├── truckconfig.json\n",
      "    ├── zeeguu_api.wsgi\n",
      "    ├── .envrc\n",
      "    ├── tools/\n",
      "    │   ├── __init__.py\n",
      "    │   ├── activity_monitor.py\n",
      "    │   ├── add_captions_from_file.py\n",
      "    │   ├── add_feed.py\n",
      "    │   ├── add_images_to_articles.py\n",
      "    │   ├── add_videos_and_captions_from_file.py\n",
      "    │   ├── analyze_classroom_recommendations.py\n",
      "    │   ├── anonymize_users.py\n",
      "    │   ├── article_crawler.py\n",
      "    │   ├── create_article_fragments_and_sources.py\n",
      "    │   ├── delete_broken_videos_from_ES.py\n",
      "    │   ├── delete_dev_users.py\n",
      "    │   ├── delete_duplicate_articles.py\n",
      "    │   ├── delete_portuguese_audio_lessons.py\n",
      "    │   ├── delete_user_deleted_articles.py\n",
      "    │   ├── download_feed.py\n",
      "    │   ├── evaluate_infer_topics.py\n",
      "    │   ├── extract_articles_with_new_topics.py\n",
      "    │   ├── extract_validation_for_topics.py\n",
      "    │   ├── feed_info.py\n",
      "    │   ├── feed_retrieval.py\n",
      "    │   ├── feeds_for_language.py\n",
      "    │   ├── fix_simplified_articles_html.py\n",
      "    │   ├── generate_audio_lesson.py\n",
      "    │   ├── get_lang_stats.py\n",
      "    │   ├── mark_broken_articles.py\n",
      "    │   ├── mysql_to_elastic_for_articles.py\n",
      "    │   ├── precompute_upcoming_meaning_lessons.py\n",
      "    │   ├── prefetch_example_sentences_for_users.py\n",
      "    │   ├── recalculate_all_word_ranks.py\n",
      "    │   ├── recalculate_video_topics.py\n",
      "    │   ├── recompute_fk_difficulties.py\n",
      "    │   ├── reduce_number_of_studied_bookmarks.py\n",
      "    │   ├── remove_sub_bookmarks_from_text.py\n",
      "    │   ├── remove_unreferenced_articles.py\n",
      "    │   ├── remove_wrong_videos.py\n",
      "    │   ├── rename_feed.py\n",
      "    │   ├── run_knn_classification_with_text.py\n",
      "    │   ├── run_knn_similarity_search.py\n",
      "    │   ├── send_subscription_emails.py\n",
      "    │   ├── set_new_topics_article.py\n",
      "    │   ├── simplify_article.py\n",
      "    │   ├── tag_advertorials.py\n",
      "    │   ├── test_user_report.py\n",
      "    │   ├── tokenizer_playground.py\n",
      "    │   ├── tokenizer_script.py\n",
      "    │   ├── transform_text_into_context.py\n",
      "    │   ├── update_article_content.py\n",
      "    │   ├── update_bookmark_pointers.py\n",
      "    │   ├── update_elastic_ids.py\n",
      "    │   ├── update_elastic_with_source_ids.py\n",
      "    │   ├── update_es_based_on_url_keyword.py\n",
      "    │   ├── update_meaning_frequencies.py\n",
      "    │   ├── users_recently_active.py\n",
      "    │   ├── validate_and_clean_examples.py\n",
      "    │   ├── video_crawler.py\n",
      "    │   ├── audio-engleza/\n",
      "    │   │   ├── basic_english_for_elderly_romanian.txt\n",
      "    │   │   ├── deploy_lessons.py\n",
      "    │   │   ├── generate_audio.py\n",
      "    │   │   ├── generate_lesson.py\n",
      "    │   │   └── generate_script.py\n",
      "    │   ├── crawl_summary/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   └── crawl_report.py\n",
      "    │   ├── migrations/\n",
      "    │   │   ├── 24-01-16--fix-portuguese-language.sql\n",
      "    │   │   ├── 24-02-rss_to_feed.sql\n",
      "    │   │   ├── 24-03-26-drop_unique_constraint_from_img_url_id.sql\n",
      "    │   │   ├── 24-03-add_img_url_to_article.sql\n",
      "    │   │   ├── 24-03-is_dev_in_users_table.sql\n",
      "    │   │   ├── 24-04-10--01--add_learning_cycle_column.sql\n",
      "    │   │   ├── 24-04-22-1-new_topic_table.sql\n",
      "    │   │   ├── 24-04-22-2-create_topic_keyword.sql\n",
      "    │   │   ├── 24-04-22-3-new_topic_filter.sql\n",
      "    │   │   ├── 24-04-22-4-new_topic_subscription.sql\n",
      "    │   │   ├── 24-04-22-5-article_topic_user_feedback.sql\n",
      "    │   │   ├── 24-04-23--02--update_learning_cycle_for_existing_users.sql\n",
      "    │   │   ├── 24-04-24--03--update_learning_cycle_preference_for_all_users.sql\n",
      "    │   │   ├── 24-05-06--update_cohort_language_to_null_for_testing.sql\n",
      "    │   │   ├── 24-05-21--add_uuid_to_session.sql\n",
      "    │   │   ├── 24-05-29--add_user_preference_to_bookmark.sql\n",
      "    │   │   ├── 24-07-01--add_article_broken_code_map.sql\n",
      "    │   │   ├── 24-07-24--add_receive_email_column.sql\n",
      "    │   │   ├── 24-07-26--remove_all_search_subscriptions_and_searches.sql\n",
      "    │   │   ├── 24-07-30--remove_articles_with_no_publish_time.sql\n",
      "    │   │   ├── 24-08-15--fix_politikens_titles.sql\n",
      "    │   │   ├── 24-08-30--user_cohort_map.sql\n",
      "    │   │   ├── 24-09-24--add_feeback_tables.sql\n",
      "    │   │   ├── 24-09-24--adding-language-to-search.sql\n",
      "    │   │   ├── 24-10-23--delete_bookmark_learned.sql\n",
      "    │   │   ├── 24-11-04--updating-url-keyword-mapping.sql\n",
      "    │   │   ├── 24-11-12-delete_old_topics.sql\n",
      "    │   │   ├── 24-11-12-rename_new_topics.sql\n",
      "    │   │   ├── 24-11-31--add_level_column.sql\n",
      "    │   │   ├── 24-12-02--add_notification_table.sql\n",
      "    │   │   ├── 24-12-02--add_user_notification_table.sql\n",
      "    │   │   ├── 25-01-06--add_index_pointers.sql\n",
      "    │   │   ├── 25-02-06--adding-cascade-deletes.sql\n",
      "    │   │   ├── 25-02-28--01-new-content-tables copy.sql\n",
      "    │   │   ├── 25-02-28--02-add-orphan-context-type.sql\n",
      "    │   │   ├── 25-02-28--03-update-user-activity-data.sql\n",
      "    │   │   ├── 25-02-28--04-delete-unused-columns-article.sql\n",
      "    │   │   ├── 25-02-28--add_video-tables.sql\n",
      "    │   │   ├── 25-04-03--update-context-type.sql\n",
      "    │   │   ├── 25-04-16--remove-vtt-from-video.sql\n",
      "    │   │   ├── 25-05-20--migrate_to_word_meaning.sql\n",
      "    │   │   ├── 25-05-21--remove_word_form.sql\n",
      "    │   │   ├── 25-05-21--rename_user_word_to_phrase.sql\n",
      "    │   │   ├── 25-05-23--removed_learning_cycle_column.sql\n",
      "    │   │   ├── 25-05-24--adding_the_user_word_table.sql\n",
      "    │   │   ├── 25-07-01--add_audio_lesson_tables.sql\n",
      "    │   │   ├── 25-07-08--add_meaning_frequency_and_phrase_type.sql\n",
      "    │   │   ├── 25-07-09--add_language_to_daily_audio_lesson.sql\n",
      "    │   │   ├── 25-07-09-a--populate_language_id_for_existing_lessons.sql\n",
      "    │   │   ├── 25-07-12--add_simplified_content_fields.sql\n",
      "    │   │   ├── 25-07-16--remove_article_duplicates.sql\n",
      "    │   │   ├── 25-07-25--add_user_article_completion_tracking.sql\n",
      "    │   │   ├── 25-07-31--add_generated_example_context.sql\n",
      "    │   │   ├── 25-07-31--recalculate_multiword_phrase_ranks.py\n",
      "    │   │   ├── 25-08-02--add_last_seen_to_users.sql\n",
      "    │   │   ├── 25-08-09--comprehensive_userword_consolidation.py\n",
      "    │   │   ├── 25-08-09--consolidate_duplicate_meanings.py\n",
      "    │   │   ├── 25-08-12--add_is_user_added_to_user_word.sql\n",
      "    │   │   ├── 25-08-13--cleanup_malformed_scroll_data.py\n",
      "    │   │   ├── 25-08-13--optimize_viewport_data_format.py\n",
      "    │   │   ├── 25-08-20--fix_ai_generator_foreign_key.sql\n",
      "    │   │   ├── 25-08-20-b--fix_remaining_ai_models_foreign_keys.sql\n",
      "    │   │   ├── 25-08-28--add_article_summary_context.sql\n",
      "    │   │   ├── 25-08-28-a--add_translation_source_to_bookmark.sql\n",
      "    │   │   ├── 25-09-08--add_hidden_column_to_user_article.sql\n",
      "    │   │   ├── 25-10-03--add_performance_indexes.sql\n",
      "    │   │   ├── 25-10-03-a--add_content_simhash_to_article.sql\n",
      "    │   │   ├── 25-10-06--add_user_article_broken_report.sql\n",
      "    │   │   └── run.sh\n",
      "    │   ├── old/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── add_all_cohorts_to_teacher.py\n",
      "    │   │   ├── add_article_id_to_text.py\n",
      "    │   │   ├── add_standard_topics.py\n",
      "    │   │   ├── add_word_rank.py\n",
      "    │   │   ├── article_recommendations.py\n",
      "    │   │   ├── assert_db_anonymized.py\n",
      "    │   │   ├── bookmark_info.py\n",
      "    │   │   ├── cleanup_non_content_bits.py\n",
      "    │   │   ├── consolidate_accounts.py\n",
      "    │   │   ├── fix_text_duplications.py\n",
      "    │   │   ├── flatten_unicode_characters.py\n",
      "    │   │   ├── forget_user.py\n",
      "    │   │   ├── learner_stats.py\n",
      "    │   │   ├── mysql_to_elastic.py\n",
      "    │   │   ├── recompute_fk_difficulties_for_polish.py\n",
      "    │   │   ├── remove_text_duplicates.py\n",
      "    │   │   ├── remove_unreferenced_articles.py\n",
      "    │   │   ├── tag_existing_articles.py\n",
      "    │   │   ├── tag_topics_in_danish.py\n",
      "    │   │   ├── update_feed.py\n",
      "    │   │   ├── update_particular_tag.py\n",
      "    │   │   └── es_v8_migration/\n",
      "    │   │       ├── migrate_old_topics_to_new_topics.py\n",
      "    │   │       ├── set_new_topics_from_url_keyword.py\n",
      "    │   │       ├── set_topic_mapping_to_keywords.py\n",
      "    │   │       ├── set_url_keywords_article.py\n",
      "    │   │       ├── url_topics.py\n",
      "    │   │       └── url_topics_count_with_pred_to_db.csv\n",
      "    │   ├── report_generator/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── data_extractor.py\n",
      "    │   │   ├── explore_report_notebook.ipynb\n",
      "    │   │   └── generate_report.py\n",
      "    │   ├── sh/\n",
      "    │   │   ├── create_test_db.sh\n",
      "    │   │   ├── live-log.sh\n",
      "    │   │   ├── live_show_exercise_outcomes.sh\n",
      "    │   │   ├── run_tests.sh\n",
      "    │   │   └── update_dock.sh\n",
      "    │   ├── sql/\n",
      "    │   │   ├── _show_foreign_keys.sql\n",
      "    │   │   ├── exercise_corectness.sql\n",
      "    │   │   ├── history_of_bookmark.sql\n",
      "    │   │   ├── most_active_users_in_the_last_year.sql\n",
      "    │   │   ├── most_recent_active_teachers.sql\n",
      "    │   │   ├── popular_exercise_outcomes.sql\n",
      "    │   │   ├── recent_exercise_outcomes.sql\n",
      "    │   │   ├── teacher-for-cohort.sql\n",
      "    │   │   ├── user_exercise_history.sql\n",
      "    │   │   ├── users_last_active_in_cohort.sql\n",
      "    │   │   └── users_with_level.sql\n",
      "    │   └── stiri-simple/\n",
      "    │       ├── deploy_to_news.py\n",
      "    │       └── generate_news_page.py\n",
      "    ├── zeeguu/\n",
      "    │   ├── __init__.py\n",
      "    │   ├── api/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── app.py\n",
      "    │   │   ├── cross_domain_app.py\n",
      "    │   │   ├── custom_fmd_graphs.py\n",
      "    │   │   ├── machine_specific.py.example\n",
      "    │   │   ├── endpoints/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── accounts.py\n",
      "    │   │   │   ├── activity_tracking.py\n",
      "    │   │   │   ├── article.py\n",
      "    │   │   │   ├── article_simplification.py\n",
      "    │   │   │   ├── audio_lessons.py\n",
      "    │   │   │   ├── bookmarks_and_words.py\n",
      "    │   │   │   ├── exercise_sessions.py\n",
      "    │   │   │   ├── exercises.py\n",
      "    │   │   │   ├── feature_toggles.py\n",
      "    │   │   │   ├── generated_examples.py\n",
      "    │   │   │   ├── nlp.py\n",
      "    │   │   │   ├── own_texts.py\n",
      "    │   │   │   ├── reading_sessions.py\n",
      "    │   │   │   ├── search.py\n",
      "    │   │   │   ├── sessions.py\n",
      "    │   │   │   ├── speech.py\n",
      "    │   │   │   ├── student.py\n",
      "    │   │   │   ├── system_languages.py\n",
      "    │   │   │   ├── topics.py\n",
      "    │   │   │   ├── translation.py\n",
      "    │   │   │   ├── user.py\n",
      "    │   │   │   ├── user_article.py\n",
      "    │   │   │   ├── user_articles.py\n",
      "    │   │   │   ├── user_languages.py\n",
      "    │   │   │   ├── user_notifications.py\n",
      "    │   │   │   ├── user_preferences.py\n",
      "    │   │   │   ├── user_statistics.py\n",
      "    │   │   │   ├── user_video.py\n",
      "    │   │   │   ├── user_watching_session.py\n",
      "    │   │   │   ├── helpers/\n",
      "    │   │   │   │   ├── __init__.py\n",
      "    │   │   │   │   └── activity_sessions.py\n",
      "    │   │   │   └── teacher_dashboard/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── _common_api_parameters.py\n",
      "    │   │   │       ├── _only_teachers_decorator.py\n",
      "    │   │   │       ├── _permissions.py\n",
      "    │   │   │       ├── article_management.py\n",
      "    │   │   │       ├── cohorts.py\n",
      "    │   │   │       ├── general.py\n",
      "    │   │   │       ├── helpers.py\n",
      "    │   │   │       ├── student.py\n",
      "    │   │   │       ├── student_exercises.py\n",
      "    │   │   │       ├── student_overviews.py\n",
      "    │   │   │       ├── student_readings.py\n",
      "    │   │   │       └── student_words.py\n",
      "    │   │   ├── test/\n",
      "    │   │   │   ├── fixtures.py\n",
      "    │   │   │   ├── test_account_creation.py\n",
      "    │   │   │   ├── test_article.py\n",
      "    │   │   │   ├── test_bookmark.py\n",
      "    │   │   │   ├── test_bookmarks.py\n",
      "    │   │   │   ├── test_endpoint_names.py\n",
      "    │   │   │   ├── test_exercise_session.py\n",
      "    │   │   │   ├── test_reading_session.py\n",
      "    │   │   │   ├── test_teacher_dashboard.py\n",
      "    │   │   │   ├── test_user_article.py\n",
      "    │   │   │   └── test_user_data.py\n",
      "    │   │   └── utils/\n",
      "    │   │       ├── __init__.py\n",
      "    │   │       ├── abort_handling.py\n",
      "    │   │       ├── caching_decorator.py\n",
      "    │   │       ├── feedparser_extensions.py\n",
      "    │   │       ├── json_result.py\n",
      "    │   │       ├── parse_json_boolean.py\n",
      "    │   │       ├── route_wrappers.py\n",
      "    │   │       └── translator.py\n",
      "    │   ├── cl/\n",
      "    │   │   └── __init__.py\n",
      "    │   ├── config/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   └── loader.py\n",
      "    │   ├── core/\n",
      "    │   │   ├── __init__.py\n",
      "    │   │   ├── constants.py\n",
      "    │   │   ├── account_management/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── user_account_creation.py\n",
      "    │   │   │   └── user_account_deletion.py\n",
      "    │   │   ├── audio_lessons/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── daily_lesson_generator.py\n",
      "    │   │   │   ├── lesson_builder.py\n",
      "    │   │   │   ├── script_generator.py\n",
      "    │   │   │   ├── voice_config.py\n",
      "    │   │   │   ├── voice_synthesizer.py\n",
      "    │   │   │   ├── word_selector.py\n",
      "    │   │   │   └── prompts/\n",
      "    │   │   │       ├── lesson_generation_prompt_v0.txt\n",
      "    │   │   │       ├── meaning_lesson--teacher_challenges_both_dialogue_and_beyond-v2.txt\n",
      "    │   │   │       ├── meaning_lesson--teacher_challenges_both_dialogue_and_beyond.txt\n",
      "    │   │   │       └── prompt_teacher_challenges_dialogue_only.txt\n",
      "    │   │   ├── behavioral_modeling/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── scroll_last_reading_percentage.py\n",
      "    │   │   ├── bookmark_quality/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── fit_for_study.py\n",
      "    │   │   │   ├── negative_qualities.py\n",
      "    │   │   │   └── positive_qualities.py\n",
      "    │   │   ├── content_cleaning/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── content_cleaner.py\n",
      "    │   │   │   └── unicode_normalization.py\n",
      "    │   │   ├── content_quality/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── advertorial_detection.py\n",
      "    │   │   │   ├── disturbing_content_detection.py\n",
      "    │   │   │   └── quality_filter.py\n",
      "    │   │   ├── content_recommender/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── elastic_recommender.py\n",
      "    │   │   ├── content_retriever/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── article_downloader.py\n",
      "    │   │   │   ├── artificial_intelligence.csv\n",
      "    │   │   │   ├── crawler_exceptions.py\n",
      "    │   │   │   ├── global_warming.csv\n",
      "    │   │   │   ├── parse_with_newspaper.py\n",
      "    │   │   │   ├── parse_with_readability_server.py\n",
      "    │   │   │   └── video_dowloader.py\n",
      "    │   │   ├── crowd_translations/\n",
      "    │   │   │   └── __init__.py\n",
      "    │   │   ├── elastic/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── basic_ops.py\n",
      "    │   │   │   ├── elastic_query_builder.py\n",
      "    │   │   │   ├── indexing.py\n",
      "    │   │   │   └── settings.py\n",
      "    │   │   ├── emailer/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── password_reset.py\n",
      "    │   │   │   ├── user_activity.py\n",
      "    │   │   │   └── zeeguu_mailer.py\n",
      "    │   │   ├── exercises/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── similar_words.py\n",
      "    │   │   ├── feed_handler/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── feed_handler.py\n",
      "    │   │   │   ├── newspaperfeed.py\n",
      "    │   │   │   └── rssfeed.py\n",
      "    │   │   ├── language/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── difficulty_estimator_factory.py\n",
      "    │   │   │   ├── difficulty_estimator_strategy.py\n",
      "    │   │   │   ├── fk_to_cefr.py\n",
      "    │   │   │   ├── services/\n",
      "    │   │   │   │   └── lingo_rank_service.py\n",
      "    │   │   │   └── strategies/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── default_difficulty_estimator.py\n",
      "    │   │   │       ├── flesch_kincaid_difficulty_estimator.py\n",
      "    │   │   │       ├── frequency_difficulty_estimator.py\n",
      "    │   │   │       └── word_rank_difficulty_estimator.py\n",
      "    │   │   ├── llm_services/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── anthropic_service.py\n",
      "    │   │   │   ├── article_simplification.py\n",
      "    │   │   │   ├── deepseek_service.py\n",
      "    │   │   │   ├── llm_service.py\n",
      "    │   │   │   ├── simplification_service.py\n",
      "    │   │   │   └── prompts/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── article_simplification.py\n",
      "    │   │   │       ├── example_generation.py\n",
      "    │   │   │       └── meaning_frequency_classifier.py\n",
      "    │   │   ├── ml_models/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── paywall_detector.py\n",
      "    │   │   │   ├── utils.py\n",
      "    │   │   │   └── binary/\n",
      "    │   │   │       ├── tfidf_multi_paywall_detect.joblib\n",
      "    │   │   │       └── tfidf_multi_paywall_detect_metadata.json\n",
      "    │   │   ├── model/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── ai_generator.py\n",
      "    │   │   │   ├── article.py\n",
      "    │   │   │   ├── article_broken_code_map.py\n",
      "    │   │   │   ├── article_difficulty_feedback.py\n",
      "    │   │   │   ├── article_fragment.py\n",
      "    │   │   │   ├── article_fragment_context.py\n",
      "    │   │   │   ├── article_summary_context.py\n",
      "    │   │   │   ├── article_title_context.py\n",
      "    │   │   │   ├── article_topic_map.py\n",
      "    │   │   │   ├── article_topic_user_feedback.py\n",
      "    │   │   │   ├── article_url_keyword_map.py\n",
      "    │   │   │   ├── audio_lesson_meaning.py\n",
      "    │   │   │   ├── bookmark.py\n",
      "    │   │   │   ├── bookmark_context.py\n",
      "    │   │   │   ├── bookmark_user_preference.py\n",
      "    │   │   │   ├── caption.py\n",
      "    │   │   │   ├── cohort.py\n",
      "    │   │   │   ├── cohort_article_map.py\n",
      "    │   │   │   ├── context_identifier.py\n",
      "    │   │   │   ├── context_type.py\n",
      "    │   │   │   ├── daily_audio_lesson.py\n",
      "    │   │   │   ├── daily_audio_lesson_segment.py\n",
      "    │   │   │   ├── daily_audio_lesson_wrapper.py\n",
      "    │   │   │   ├── db.py\n",
      "    │   │   │   ├── difficulty_lingo_rank.py\n",
      "    │   │   │   ├── domain_name.py\n",
      "    │   │   │   ├── example_sentence.py\n",
      "    │   │   │   ├── example_sentence_context.py\n",
      "    │   │   │   ├── exercise.py\n",
      "    │   │   │   ├── exercise_outcome.py\n",
      "    │   │   │   ├── exercise_source.py\n",
      "    │   │   │   ├── feed.py\n",
      "    │   │   │   ├── feedback_component.py\n",
      "    │   │   │   ├── language.py\n",
      "    │   │   │   ├── meaning.py\n",
      "    │   │   │   ├── meaning_frequency_classifier.py\n",
      "    │   │   │   ├── new_text.py\n",
      "    │   │   │   ├── notification.py\n",
      "    │   │   │   ├── personal_copy.py\n",
      "    │   │   │   ├── phrase.py\n",
      "    │   │   │   ├── search.py\n",
      "    │   │   │   ├── search_filter.py\n",
      "    │   │   │   ├── search_subscription.py\n",
      "    │   │   │   ├── session.py\n",
      "    │   │   │   ├── sorted_exercise_log.py\n",
      "    │   │   │   ├── source.py\n",
      "    │   │   │   ├── source_text.py\n",
      "    │   │   │   ├── source_type.py\n",
      "    │   │   │   ├── starred_article.py\n",
      "    │   │   │   ├── teacher.py\n",
      "    │   │   │   ├── teacher_cohort_map.py\n",
      "    │   │   │   ├── text.py\n",
      "    │   │   │   ├── topic.py\n",
      "    │   │   │   ├── topic_filter.py\n",
      "    │   │   │   ├── topic_subscription.py\n",
      "    │   │   │   ├── unique_code.py\n",
      "    │   │   │   ├── url.py\n",
      "    │   │   │   ├── url_keyword.py\n",
      "    │   │   │   ├── user.py\n",
      "    │   │   │   ├── user_activitiy_data.py\n",
      "    │   │   │   ├── user_article.py\n",
      "    │   │   │   ├── user_article_broken_report.py\n",
      "    │   │   │   ├── user_cohort_map.py\n",
      "    │   │   │   ├── user_exercise_session.py\n",
      "    │   │   │   ├── user_feedback.py\n",
      "    │   │   │   ├── user_language.py\n",
      "    │   │   │   ├── user_notification.py\n",
      "    │   │   │   ├── user_preference.py\n",
      "    │   │   │   ├── user_reading_session.py\n",
      "    │   │   │   ├── user_video.py\n",
      "    │   │   │   ├── user_watching_session.py\n",
      "    │   │   │   ├── user_word.py\n",
      "    │   │   │   ├── video.py\n",
      "    │   │   │   ├── video_caption_context.py\n",
      "    │   │   │   ├── video_tag.py\n",
      "    │   │   │   ├── video_tag_map.py\n",
      "    │   │   │   ├── video_title_context.py\n",
      "    │   │   │   ├── video_topic_map.py\n",
      "    │   │   │   └── yt_channel.py\n",
      "    │   │   ├── nlp_pipeline/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── alignment_errant.py\n",
      "    │   │   │   ├── automatic_gec_tagging.py\n",
      "    │   │   │   ├── confusion_generator.py\n",
      "    │   │   │   ├── edit_errant.py\n",
      "    │   │   │   ├── reduce_context.py\n",
      "    │   │   │   └── spacy_wrapper.py\n",
      "    │   │   ├── reading_analysis/\n",
      "    │   │   │   └── macro_reading_session.py\n",
      "    │   │   ├── semantic_search/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── elastic_semantic_search.py\n",
      "    │   │   ├── semantic_vector_api/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── retrieve_embeddings.py\n",
      "    │   │   ├── sql/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── query_building.py\n",
      "    │   │   │   ├── learner/\n",
      "    │   │   │   │   ├── __init__.py\n",
      "    │   │   │   │   ├── exercises_history.py\n",
      "    │   │   │   │   └── words.py\n",
      "    │   │   │   ├── queries/\n",
      "    │   │   │   │   ├── query_loader.py\n",
      "    │   │   │   │   └── words_to_study.sql\n",
      "    │   │   │   └── teacher/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       └── teachers_for_cohort.py\n",
      "    │   │   ├── test/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── mocking_the_web.py\n",
      "    │   │   │   ├── model_test_mixin.py\n",
      "    │   │   │   ├── test_article.py\n",
      "    │   │   │   ├── test_bookmark.py\n",
      "    │   │   │   ├── test_cohort.py\n",
      "    │   │   │   ├── test_difficulty_estimator_factory.py\n",
      "    │   │   │   ├── test_domain.py\n",
      "    │   │   │   ├── test_feed.py\n",
      "    │   │   │   ├── test_language.py\n",
      "    │   │   │   ├── test_logging.py\n",
      "    │   │   │   ├── test_retrieve_and_compute.py\n",
      "    │   │   │   ├── test_scheduling.py\n",
      "    │   │   │   ├── test_text.py\n",
      "    │   │   │   ├── test_tokenizer.py\n",
      "    │   │   │   ├── test_url.py\n",
      "    │   │   │   ├── test_url_keywords.py\n",
      "    │   │   │   ├── test_user.py\n",
      "    │   │   │   ├── test_user_article.py\n",
      "    │   │   │   ├── test_user_article_broken_report.py\n",
      "    │   │   │   ├── test_user_preferences.py\n",
      "    │   │   │   ├── test_user_word.py\n",
      "    │   │   │   ├── testing_data.py\n",
      "    │   │   │   ├── rules/\n",
      "    │   │   │   │   ├── __init__.py\n",
      "    │   │   │   │   ├── article_rule.py\n",
      "    │   │   │   │   ├── base_rule.py\n",
      "    │   │   │   │   ├── bookmark_context_rule.py\n",
      "    │   │   │   │   ├── bookmark_rule.py\n",
      "    │   │   │   │   ├── cohort_rule.py\n",
      "    │   │   │   │   ├── exercise_rule.py\n",
      "    │   │   │   │   ├── exercise_session_rule.py\n",
      "    │   │   │   │   ├── exercise_source_rule.py\n",
      "    │   │   │   │   ├── feed_rule.py\n",
      "    │   │   │   │   ├── language_rule.py\n",
      "    │   │   │   │   ├── meaning_rule.py\n",
      "    │   │   │   │   ├── new_text_rule.py\n",
      "    │   │   │   │   ├── outcome_rule.py\n",
      "    │   │   │   │   ├── phrase_rule.py\n",
      "    │   │   │   │   ├── scheduler_rule.py\n",
      "    │   │   │   │   ├── source_rule.py\n",
      "    │   │   │   │   ├── source_text_rule.py\n",
      "    │   │   │   │   ├── text_rule.py\n",
      "    │   │   │   │   ├── topic_rule.py\n",
      "    │   │   │   │   ├── url_rule.py\n",
      "    │   │   │   │   ├── user_article_rule.py\n",
      "    │   │   │   │   ├── user_exercise_session_rule.py\n",
      "    │   │   │   │   ├── user_reading_session_rule.py\n",
      "    │   │   │   │   ├── user_rule.py\n",
      "    │   │   │   │   └── user_word_rule.py\n",
      "    │   │   │   ├── test_data/\n",
      "    │   │   │   │   ├── blinden_und_elefant.html\n",
      "    │   │   │   │   ├── cnn_kathmandu.html\n",
      "    │   │   │   │   ├── cnn_kathmandu.json\n",
      "    │   │   │   │   ├── cnn_kathmandu.txt\n",
      "    │   │   │   │   ├── der_kleine_prinz.html\n",
      "    │   │   │   │   ├── der_kleine_prinz.json\n",
      "    │   │   │   │   ├── der_kleine_prinz.txt\n",
      "    │   │   │   │   ├── faz_leichtathletik.html\n",
      "    │   │   │   │   ├── faz_leichtathletik.json\n",
      "    │   │   │   │   ├── faz_leichtathletik.txt\n",
      "    │   │   │   │   ├── jp_article_example.html\n",
      "    │   │   │   │   ├── jp_article_example.json\n",
      "    │   │   │   │   ├── jp_article_example.txt\n",
      "    │   │   │   │   ├── lemonde_formation.html\n",
      "    │   │   │   │   ├── lemonde_formation.json\n",
      "    │   │   │   │   ├── lemonde_formation.txt\n",
      "    │   │   │   │   ├── lemonde_vols_americans.html\n",
      "    │   │   │   │   ├── newscientist_fish.html\n",
      "    │   │   │   │   ├── onion_us_military.html\n",
      "    │   │   │   │   ├── propublica_investing.html\n",
      "    │   │   │   │   ├── public_suffix_list.dat\n",
      "    │   │   │   │   ├── random_emails.txt\n",
      "    │   │   │   │   ├── random_urls.txt\n",
      "    │   │   │   │   ├── spiegel.rss\n",
      "    │   │   │   │   ├── spiegel_nancy.html\n",
      "    │   │   │   │   ├── spiegel_nancy.json\n",
      "    │   │   │   │   ├── spiegel_nancy.txt\n",
      "    │   │   │   │   ├── spiegel_venezuela.html\n",
      "    │   │   │   │   ├── spiegel_venezuela.json\n",
      "    │   │   │   │   ├── spiegel_venezuela.txt\n",
      "    │   │   │   │   ├── verdensbedste.html\n",
      "    │   │   │   │   ├── verdensbedste_indonesien.html\n",
      "    │   │   │   │   ├── verdensbedste_indonesien.json\n",
      "    │   │   │   │   ├── verdensbedste_indonesien.txt\n",
      "    │   │   │   │   ├── verdensbedste_jorde.html\n",
      "    │   │   │   │   ├── verdensbedste_jorde.json\n",
      "    │   │   │   │   └── verdensbedste_jorde.txt\n",
      "    │   │   │   └── tests_difficulty_estimator_strategies/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── test_default_difficulty_estimator.py\n",
      "    │   │   │       ├── test_flesch_kincaid_difficulty_estimator.py\n",
      "    │   │   │       └── test_frequency_difficulty_estimator.py\n",
      "    │   │   ├── tokenization/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── nltk_tokenizer.py\n",
      "    │   │   │   ├── stanza_tokenizer.py\n",
      "    │   │   │   ├── token.py\n",
      "    │   │   │   ├── word_position_finder.py\n",
      "    │   │   │   └── zeeguu_tokenizer.py\n",
      "    │   │   ├── user_activity_hooks/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── article_interaction_hooks.py\n",
      "    │   │   ├── user_statistics/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── activity.py\n",
      "    │   │   │   ├── exercise_corectness.py\n",
      "    │   │   │   ├── exercise_sessions.py\n",
      "    │   │   │   ├── reading_sessions.py\n",
      "    │   │   │   └── student_overview.py\n",
      "    │   │   ├── util/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── compute_fk_word_count.py\n",
      "    │   │   │   ├── encoding.py\n",
      "    │   │   │   ├── hash.py\n",
      "    │   │   │   ├── list.py\n",
      "    │   │   │   ├── reading_time_estimator.py\n",
      "    │   │   │   ├── text.py\n",
      "    │   │   │   ├── time.py\n",
      "    │   │   │   ├── time_conversion.py\n",
      "    │   │   │   └── timer_logging_decorator.py\n",
      "    │   │   ├── word_filter/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   ├── profanity_filter.py\n",
      "    │   │   │   ├── proper_noun_filter.py\n",
      "    │   │   │   └── data/\n",
      "    │   │   │       ├── city-names.txt\n",
      "    │   │   │       ├── person-names.txt\n",
      "    │   │   │       └── bad-words/\n",
      "    │   │   │           ├── ar\n",
      "    │   │   │           ├── cs\n",
      "    │   │   │           ├── da\n",
      "    │   │   │           ├── de\n",
      "    │   │   │           ├── en\n",
      "    │   │   │           ├── eo\n",
      "    │   │   │           ├── es\n",
      "    │   │   │           ├── fa\n",
      "    │   │   │           ├── fi\n",
      "    │   │   │           ├── fil\n",
      "    │   │   │           ├── fr\n",
      "    │   │   │           ├── fr-CA-u-sd-caqc\n",
      "    │   │   │           ├── hi\n",
      "    │   │   │           ├── hu\n",
      "    │   │   │           ├── it\n",
      "    │   │   │           ├── ja\n",
      "    │   │   │           ├── kab\n",
      "    │   │   │           ├── ko\n",
      "    │   │   │           ├── LICENSE\n",
      "    │   │   │           ├── nl\n",
      "    │   │   │           ├── no\n",
      "    │   │   │           ├── pl\n",
      "    │   │   │           ├── pt\n",
      "    │   │   │           ├── ru\n",
      "    │   │   │           ├── sv\n",
      "    │   │   │           ├── th\n",
      "    │   │   │           ├── tlh\n",
      "    │   │   │           ├── tr\n",
      "    │   │   │           └── zh\n",
      "    │   │   ├── word_scheduling/\n",
      "    │   │   │   ├── __init__.py\n",
      "    │   │   │   └── basicSR/\n",
      "    │   │   │       ├── __init__.py\n",
      "    │   │   │       ├── basicSR.py\n",
      "    │   │   │       └── four_levels_per_word.py\n",
      "    │   │   ├── word_stats/\n",
      "    │   │   │   └── __init__.py\n",
      "    │   │   └── youtube_api/\n",
      "    │   │       └── youtube_api.py\n",
      "    │   └── logging/\n",
      "    │       └── __init__.py\n",
      "    ├── .githooks/\n",
      "    │   ├── pre-commit\n",
      "    │   └── rules/\n",
      "    │       └── routes_should_not_end_in_slash\n",
      "    └── .github/\n",
      "        └── workflows/\n",
      "            ├── publish_docker_image.yml\n",
      "            ├── render-architectural-diff.yml\n",
      "            └── test.yml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "## clone repository for testing\n",
    "\n",
    "tool_input = {\n",
    "    \"repo_url\": \"https://github.com/zeeguu/api\",\n",
    "    \"dest\": \"zeeguu-api\",\n",
    "}\n",
    "\n",
    "if False:\n",
    "    output = git_clone_tool.invoke(tool_input)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "## output repository details\n",
    "tool_input = {\n",
    "    \"local_repository_path\": \"./repositories/zeeguu-api\",\n",
    "    \"output_path\": \"./repositories/zeeguu-api/repo_extraction.txt\",\n",
    "}\n",
    "\n",
    "if False:\n",
    "    output = await extract_repository_details.ainvoke(tool_input)\n",
    "\n",
    "    print(output[\"summary\"])\n",
    "    print(output[\"tree\"])\n",
    "\n",
    "queries = []\n",
    "\n",
    "tools = [list_current_directory, change_directory, current_working_directory, get_parent_and_child_directory, extract_repository_details]\n",
    "tool_descriptions = [tool.description for tool in tools]\n",
    "agent = create_agent(\n",
    "    \"openai:gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    prompt=f\"Act as an assistant, that can navigate the file system using the tools provided.\\\n",
    "    The available tools are: {tool_descriptions}\"\n",
    ")\n",
    "\n",
    "repo_to_clone = \"https://github.com/zeeguu/api\"\n",
    "clone = agent.invoke(\n",
    "    f\"Can you clone the following github repository: {repo_to_clone}, feel free to overwrite if a clone already exists, \"\n",
    ")\n",
    "repo_extraction = agent.invoke(\n",
    "    f\"Can you extract the repository details of the cloned repository in, and save the output to the root folder of zeeguu-api/repo_extraction.txt\"\n",
    ")\n",
    "\n",
    "cwd = agent.invoke({\"messages\": [HumanMessage(\"Can you list the current directory?\")]})\n",
    "tree = agent.invoke({\"messages\": [HumanMessage(\"Can you get the directory tree from the repository?\")]})\n",
    "chdir = agent.invoke({\"messages\": [HumanMessage(\"Can you navigate to the audio-engleza folder?\")]})\n",
    "info = agent.invoke({\"messages\": [HumanMessage(\"Can you show me the parent and child directories?\")]})\n",
    "archlens_init = agent.invoke({\"messages\": [HumanMessage(\"Can you initialize archLens in the cloned repository?\")]})\n",
    "archlens_render = agent.invoke({\"messages\": [HumanMessage(\"Can you render an archLens view of the repository?\")]})\n",
    "\n",
    "queries += [clone, repo_extraction, cwd, tree, chdir, info, archlens_init, archlens_render]\n",
    "\n",
    "for query in queries:\n",
    "    for message in query[\"message\"]:\n",
    "        message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a89d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool('run_archlens')\n",
    "def run_archlens(path: str):\n",
    "    \"\"\"\"Run archLens from the <path>.\n",
    "    \n",
    "    args:\n",
    "    - path: The path to the repository to run archLens on.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    change_directory(path)\n",
    "      \n",
    "    if not os.path.exists(\"archlens.json\"):\n",
    "        return \"archlens.json does not exist. Please run init_archLens first.\"\n",
    "\n",
    "    exit_code = os.system(f\"archlens render\")\n",
    "\n",
    "    return {\"message\": \"Ran archLens\", \"exit_code\": exit_code}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f8352",
   "metadata": {},
   "source": [
    "## Create navigation class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arch-reconstruct-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
